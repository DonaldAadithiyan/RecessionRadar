{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def feature_selection_pipeline(train_df, test_df, n_features_target=35):\n",
    "    \"\"\"\n",
    "    Feature selection pipeline using RFECV for multiple target variables.\n",
    "    Keeps recession probabilities and targets, selects best additional features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_df : pd.DataFrame\n",
    "        Training dataset with engineered features\n",
    "    test_df : pd.DataFrame\n",
    "        Test dataset with engineered features\n",
    "    n_features_target : int\n",
    "        Target number of features to keep (default: 35)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (train_selected, test_selected, selection_results)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"FEATURE SELECTION PIPELINE - RFECV\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Make copies\n",
    "    train_fs = train_df.copy()\n",
    "    test_fs = test_df.copy()\n",
    "    \n",
    "    # Define column groups\n",
    "    mandatory_cols = ['date']  # Always keep\n",
    "    \n",
    "    recession_cols = [\n",
    "        'recession_probability', \n",
    "        '1_month_recession_probability',\n",
    "        '3_month_recession_probability', \n",
    "        '6_month_recession_probability'\n",
    "    ]\n",
    "    \n",
    "    target_cols = [\n",
    "        '1_year_rate', '3_months_rate', '6_months_rate', 'CPI', 'INDPRO', \n",
    "        '10_year_rate', 'share_price', 'unemployment_rate', 'PPI', \n",
    "        'OECD_CLI_index', 'CSI_index', 'gdp_per_capita'\n",
    "    ]\n",
    "    \n",
    "    # Must-keep columns (not subject to feature selection)\n",
    "    must_keep = mandatory_cols + recession_cols + target_cols\n",
    "    n_must_keep = len(must_keep)\n",
    "    \n",
    "    print(f\"Dataset shape: Train {train_fs.shape}, Test {test_fs.shape}\")\n",
    "    print(f\"Must-keep columns: {n_must_keep} ({must_keep})\")\n",
    "    \n",
    "    # Available features for selection (exclude must-keep columns)\n",
    "    available_features = [col for col in train_fs.columns if col not in must_keep]\n",
    "    n_available = len(available_features)\n",
    "    \n",
    "    # Calculate how many additional features we can select\n",
    "    n_additional_target = max(0, n_features_target - n_must_keep)\n",
    "    n_additional_actual = min(n_additional_target, n_available)\n",
    "    \n",
    "    print(f\"Available features for selection: {n_available}\")\n",
    "    print(f\"Target additional features: {n_additional_target}\")\n",
    "    print(f\"Actual additional features to select: {n_additional_actual}\")\n",
    "    print(f\"Final target columns: {n_must_keep + n_additional_actual}\")\n",
    "    \n",
    "    if n_additional_actual <= 0:\n",
    "        print(\"\\nWARNING: No additional features to select!\")\n",
    "        selected_features = must_keep\n",
    "    else:\n",
    "        print(f\"\\nAvailable features for selection:\")\n",
    "        for i, col in enumerate(available_features[:20], 1):\n",
    "            print(f\"  {i:2d}. {col}\")\n",
    "        if n_available > 20:\n",
    "            print(f\"  ... and {n_available - 20} more features\")\n",
    "    \n",
    "    # Initialize results storage\n",
    "    selection_results = {\n",
    "        'must_keep_cols': must_keep,\n",
    "        'available_features': available_features,\n",
    "        'target_additional_features': n_additional_actual,\n",
    "        'rfecv_results': {},\n",
    "        'feature_importance_summary': {},\n",
    "        'final_selected_features': [],\n",
    "        'categorical_encoders': {}\n",
    "    }\n",
    "    \n",
    "    # ==========================================\n",
    "    # RFECV FEATURE SELECTION\n",
    "    # ==========================================\n",
    "    \n",
    "    if n_additional_actual > 0:\n",
    "        print(f\"\\n\" + \"=\" * 50)\n",
    "        print(\"RECURSIVE FEATURE ELIMINATION WITH CV\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Prepare data for RFECV\n",
    "        X_available = train_fs[available_features].copy()\n",
    "        \n",
    "        # Handle missing values and categorical variables\n",
    "        print(f\"\\nPreparing data for RFECV...\")\n",
    "        print(f\"Missing values in available features: {X_available.isnull().sum().sum()}\")\n",
    "        \n",
    "        # Identify and encode categorical variables\n",
    "        categorical_cols = []\n",
    "        for col in X_available.columns:\n",
    "            if X_available[col].dtype == 'object' or X_available[col].dtype.name == 'category':\n",
    "                categorical_cols.append(col)\n",
    "        \n",
    "        print(f\"Categorical columns found: {categorical_cols}\")\n",
    "        \n",
    "        # Handle categorical variables with label encoding\n",
    "        label_encoders = {}\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            # Handle missing values first\n",
    "            X_available[col] = X_available[col].fillna('Unknown')\n",
    "            X_available[col] = le.fit_transform(X_available[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "        \n",
    "        # Store encoders for later use\n",
    "        selection_results['categorical_encoders'] = label_encoders\n",
    "        \n",
    "        # Handle missing values for numerical columns\n",
    "        X_available = X_available.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        # If still missing values, fill with median\n",
    "        for col in X_available.columns:\n",
    "            if X_available[col].isnull().any():\n",
    "                X_available[col] = X_available[col].fillna(X_available[col].median())\n",
    "        \n",
    "        print(f\"After preprocessing: {X_available.isnull().sum().sum()} missing values\")\n",
    "        \n",
    "        # Feature importance aggregation across targets\n",
    "        feature_scores = np.zeros(len(available_features))\n",
    "        feature_selection_frequency = np.zeros(len(available_features))\n",
    "        \n",
    "        # RFECV parameters optimized for small dataset\n",
    "        cv_folds = min(3, len(train_fs) // 100)  # 3 folds for 630 rows\n",
    "        cv_folds = max(cv_folds, 2)  # At least 2 folds\n",
    "        \n",
    "        rf_params = {\n",
    "            'n_estimators': 50,  # Reduced for speed\n",
    "            'max_depth': 8,      # Limited depth for small dataset\n",
    "            'min_samples_split': 10,  # Higher to prevent overfitting\n",
    "            'min_samples_leaf': 5,    # Higher to prevent overfitting\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        print(f\"RFECV Configuration:\")\n",
    "        print(f\"  CV folds: {cv_folds}\")\n",
    "        print(f\"  RandomForest params: {rf_params}\")\n",
    "        print(f\"  Target additional features: {n_additional_actual}\")\n",
    "        \n",
    "        # Run RFECV for each target variable\n",
    "        successful_targets = 0\n",
    "        for i, target_col in enumerate(target_cols, 1):\n",
    "            print(f\"\\n[{i}/{len(target_cols)}] Processing target: {target_col}\")\n",
    "            \n",
    "            if target_col not in train_fs.columns:\n",
    "                print(f\"  WARNING: {target_col} not found in training data, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare target variable\n",
    "            y_target = train_fs[target_col].copy()\n",
    "            \n",
    "            # Handle missing values in target\n",
    "            mask_valid = ~y_target.isnull()\n",
    "            if mask_valid.sum() < 50:  # Need at least 50 valid samples\n",
    "                print(f\"  WARNING: Insufficient valid samples for {target_col} ({mask_valid.sum()}), skipping...\")\n",
    "                continue\n",
    "            \n",
    "            X_target = X_available[mask_valid]\n",
    "            y_target = y_target[mask_valid]\n",
    "            \n",
    "            print(f\"  Valid samples: {len(y_target)}\")\n",
    "            \n",
    "            try:\n",
    "                # Create RFECV\n",
    "                rf_estimator = RandomForestRegressor(**rf_params)\n",
    "                \n",
    "                rfecv = RFECV(\n",
    "                    estimator=rf_estimator,\n",
    "                    min_features_to_select=min(5, n_additional_actual),  # At least 5 features\n",
    "                    cv=KFold(n_splits=cv_folds, shuffle=True, random_state=42),\n",
    "                    scoring='r2',\n",
    "                    n_jobs=1  # Avoid nested parallelism issues\n",
    "                )\n",
    "                \n",
    "                # Fit RFECV\n",
    "                print(f\"  Running RFECV...\")\n",
    "                rfecv.fit(X_target, y_target)\n",
    "                \n",
    "                # Get CV scores - handle both old and new scikit-learn versions\n",
    "                try:\n",
    "                    cv_scores = rfecv.cv_results_  # New version\n",
    "                except AttributeError:\n",
    "                    cv_scores = rfecv.grid_scores_  # Old version\n",
    "                \n",
    "                # Store results\n",
    "                selection_results['rfecv_results'][target_col] = {\n",
    "                    'optimal_features': rfecv.n_features_,\n",
    "                    'selected_features': np.array(available_features)[rfecv.support_].tolist(),\n",
    "                    'feature_ranking': rfecv.ranking_.copy(),\n",
    "                    'cv_scores': cv_scores.copy() if hasattr(cv_scores, 'copy') else list(cv_scores),\n",
    "                    'support_mask': rfecv.support_.copy()\n",
    "                }\n",
    "                \n",
    "                # Aggregate feature importance\n",
    "                feature_scores += (1.0 / rfecv.ranking_)  # Higher score for lower ranking\n",
    "                feature_selection_frequency += rfecv.support_.astype(float)\n",
    "                successful_targets += 1\n",
    "                \n",
    "                # Get best CV score - handle different data types\n",
    "                if hasattr(cv_scores, '__iter__') and len(cv_scores) > 0:\n",
    "                    try:\n",
    "                        best_cv_score = max(cv_scores)\n",
    "                        if isinstance(best_cv_score, (int, float)):\n",
    "                            score_str = f\"{best_cv_score:.4f}\"\n",
    "                        else:\n",
    "                            score_str = str(best_cv_score)\n",
    "                    except (ValueError, TypeError):\n",
    "                        score_str = \"N/A\"\n",
    "                else:\n",
    "                    try:\n",
    "                        if isinstance(cv_scores, (int, float)):\n",
    "                            score_str = f\"{cv_scores:.4f}\"\n",
    "                        else:\n",
    "                            score_str = str(cv_scores)\n",
    "                    except:\n",
    "                        score_str = \"N/A\"\n",
    "                \n",
    "                print(f\"  Optimal features: {rfecv.n_features_}\")\n",
    "                print(f\"  Best CV score: {score_str}\")\n",
    "                print(f\"  Selected features: {rfecv.support_.sum()}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR in RFECV for {target_col}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # ==========================================\n",
    "        # AGGREGATE FEATURE SELECTION\n",
    "        # ==========================================\n",
    "        print(f\"\\n\" + \"=\" * 50)\n",
    "        print(\"AGGREGATING FEATURE SELECTION RESULTS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Calculate aggregate scores\n",
    "        if successful_targets > 0:\n",
    "            # Normalize scores\n",
    "            avg_importance = feature_scores / successful_targets\n",
    "            avg_selection_freq = feature_selection_frequency / successful_targets\n",
    "            \n",
    "            # Combine importance and selection frequency\n",
    "            combined_score = 0.7 * avg_importance + 0.3 * avg_selection_freq\n",
    "            \n",
    "            # Create feature summary\n",
    "            feature_summary = pd.DataFrame({\n",
    "                'feature': available_features,\n",
    "                'avg_importance': avg_importance,\n",
    "                'selection_frequency': avg_selection_freq,\n",
    "                'combined_score': combined_score\n",
    "            })\n",
    "            \n",
    "            # Sort by combined score\n",
    "            feature_summary = feature_summary.sort_values('combined_score', ascending=False)\n",
    "            \n",
    "            # Select top features\n",
    "            top_features = feature_summary.head(n_additional_actual)['feature'].tolist()\n",
    "            \n",
    "            print(f\"Processed {successful_targets} target variables successfully\")\n",
    "            print(f\"Selected top {len(top_features)} features:\")\n",
    "            \n",
    "            for i, feat in enumerate(top_features[:min(15, len(top_features))], 1):\n",
    "                feature_row = feature_summary[feature_summary['feature'] == feat].iloc[0]\n",
    "                print(f\"  {i:2d}. {feat}\")\n",
    "                print(f\"      Importance: {feature_row['avg_importance']:.4f}, \"\n",
    "                      f\"Frequency: {feature_row['selection_frequency']:.2f}, \"\n",
    "                      f\"Combined: {feature_row['combined_score']:.4f}\")\n",
    "            \n",
    "            if len(top_features) > 15:\n",
    "                print(f\"  ... and {len(top_features) - 15} more features\")\n",
    "            \n",
    "            # Store results\n",
    "            selection_results['feature_importance_summary'] = feature_summary\n",
    "            selected_additional_features = top_features\n",
    "            \n",
    "        else:\n",
    "            print(\"ERROR: No targets were successfully processed!\")\n",
    "            # Fallback: select features based on correlation with recession probability\n",
    "            print(\"\\nFalling back to correlation-based selection...\")\n",
    "            \n",
    "            try:\n",
    "                # Use correlation with recession_probability as fallback\n",
    "                correlation_scores = []\n",
    "                recession_target = train_fs['recession_probability'].fillna(method='ffill').fillna(method='bfill')\n",
    "                \n",
    "                for col in available_features:\n",
    "                    col_data = X_available[col]\n",
    "                    \n",
    "                    # Calculate correlation\n",
    "                    corr = np.corrcoef(col_data, recession_target)[0, 1]\n",
    "                    correlation_scores.append(abs(corr) if not np.isnan(corr) else 0)\n",
    "                \n",
    "                # Create fallback feature summary\n",
    "                feature_summary = pd.DataFrame({\n",
    "                    'feature': available_features,\n",
    "                    'correlation_score': correlation_scores\n",
    "                })\n",
    "                \n",
    "                feature_summary = feature_summary.sort_values('correlation_score', ascending=False)\n",
    "                top_features = feature_summary.head(n_additional_actual)['feature'].tolist()\n",
    "                \n",
    "                print(f\"Selected top {len(top_features)} features by correlation:\")\n",
    "                for i, feat in enumerate(top_features[:10], 1):\n",
    "                    corr_score = feature_summary[feature_summary['feature'] == feat]['correlation_score'].iloc[0]\n",
    "                    print(f\"  {i:2d}. {feat} (corr: {corr_score:.4f})\")\n",
    "                \n",
    "                selection_results['feature_importance_summary'] = feature_summary\n",
    "                selected_additional_features = top_features\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ERROR in fallback selection: {str(e)}\")\n",
    "                selected_additional_features = available_features[:n_additional_actual]\n",
    "    \n",
    "    else:\n",
    "        selected_additional_features = []\n",
    "    \n",
    "    # ==========================================\n",
    "    # FINAL FEATURE SET\n",
    "    # ==========================================\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"FINAL FEATURE SELECTION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Combine must-keep and selected features\n",
    "    final_features = must_keep + selected_additional_features\n",
    "    selection_results['final_selected_features'] = final_features\n",
    "    \n",
    "    print(f\"Final feature set ({len(final_features)} features):\")\n",
    "    print(f\"  Must-keep: {len(must_keep)} features\")\n",
    "    print(f\"  Selected: {len(selected_additional_features)} features\")\n",
    "    \n",
    "    # Show feature breakdown\n",
    "    print(f\"\\nFeature breakdown:\")\n",
    "    print(f\"  Mandatory: {len(mandatory_cols)} - {mandatory_cols}\")\n",
    "    print(f\"  Recession: {len(recession_cols)} - {recession_cols}\")\n",
    "    print(f\"  Targets: {len(target_cols)} - {target_cols}\")\n",
    "    print(f\"  Selected: {len(selected_additional_features)}\")\n",
    "    \n",
    "    if selected_additional_features:\n",
    "        print(f\"    Top selected features:\")\n",
    "        for i, feat in enumerate(selected_additional_features[:10], 1):\n",
    "            print(f\"      {i:2d}. {feat}\")\n",
    "        if len(selected_additional_features) > 10:\n",
    "            print(f\"      ... and {len(selected_additional_features) - 10} more\")\n",
    "    \n",
    "    # Apply feature selection to both train and test\n",
    "    train_selected = train_fs[final_features].copy()\n",
    "    test_selected = test_fs[final_features].copy()\n",
    "    \n",
    "    # Apply categorical encoding to test data if needed\n",
    "    if selection_results['categorical_encoders']:\n",
    "        print(f\"\\nApplying categorical encoding to test data...\")\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in selected_additional_features and col in test_selected.columns:\n",
    "                # Handle test data encoding\n",
    "                test_values = test_selected[col].fillna('Unknown').astype(str)\n",
    "                le = selection_results['categorical_encoders'][col]\n",
    "                \n",
    "                # Map known categories, unknown ones become -1\n",
    "                encoded_values = []\n",
    "                for val in test_values:\n",
    "                    try:\n",
    "                        encoded_val = le.transform([val])[0]\n",
    "                    except ValueError:\n",
    "                        # New category not seen in training, use most frequent class\n",
    "                        encoded_val = 0  # or use mode of training data\n",
    "                    encoded_values.append(encoded_val)\n",
    "                \n",
    "                test_selected[col] = encoded_values\n",
    "                train_selected[col] = selection_results['categorical_encoders'][col].transform(\n",
    "                    train_selected[col].fillna('Unknown').astype(str)\n",
    "                )\n",
    "    \n",
    "    print(f\"\\nFinal dataset shapes:\")\n",
    "    print(f\"  Training: {train_selected.shape}\")\n",
    "    print(f\"  Test: {test_selected.shape}\")\n",
    "    \n",
    "    # Verify no missing columns\n",
    "    missing_in_test = [col for col in final_features if col not in test_fs.columns]\n",
    "    if missing_in_test:\n",
    "        print(f\"  WARNING: Missing in test data: {missing_in_test}\")\n",
    "    \n",
    "    # Data quality check\n",
    "    print(f\"\\nData quality after selection:\")\n",
    "    train_missing = train_selected.isnull().sum().sum()\n",
    "    test_missing = test_selected.isnull().sum().sum()\n",
    "    print(f\"  Training missing values: {train_missing}\")\n",
    "    print(f\"  Test missing values: {test_missing}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"FEATURE SELECTION COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return train_selected, test_selected, selection_results\n",
    "\n",
    "# Utility function to analyze selection results\n",
    "def analyze_selection_results(selection_results):\n",
    "    \"\"\"\n",
    "    Analyze and visualize feature selection results\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FEATURE SELECTION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if ('feature_importance_summary' in selection_results and \n",
    "        isinstance(selection_results['feature_importance_summary'], pd.DataFrame) and \n",
    "        not selection_results['feature_importance_summary'].empty):\n",
    "        \n",
    "        summary = selection_results['feature_importance_summary']\n",
    "        \n",
    "        print(f\"\\nTop 15 features by combined score:\")\n",
    "        top_15 = summary.head(15)\n",
    "        for i, (_, row) in enumerate(top_15.iterrows(), 1):\n",
    "            if 'combined_score' in row:\n",
    "                print(f\"{i:2d}. {row['feature']:<30} \"\n",
    "                      f\"Score: {row['combined_score']:.4f} \"\n",
    "                      f\"(Imp: {row['avg_importance']:.3f}, \"\n",
    "                      f\"Freq: {row['selection_frequency']:.2f})\")\n",
    "            else:\n",
    "                # Fallback correlation-based selection\n",
    "                print(f\"{i:2d}. {row['feature']:<30} \"\n",
    "                      f\"Correlation: {row['correlation_score']:.4f}\")\n",
    "        \n",
    "        if 'selection_frequency' in summary.columns:\n",
    "            print(f\"\\nSelection frequency distribution:\")\n",
    "            freq_dist = summary['selection_frequency'].value_counts().sort_index(ascending=False)\n",
    "            for freq, count in freq_dist.items():\n",
    "                if freq > 0:\n",
    "                    targets_processed = len(selection_results['rfecv_results'])\n",
    "                    print(f\"  Selected by {freq:.0f}/{targets_processed} targets: {count} features\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nNo feature importance summary available.\")\n",
    "    \n",
    "    print(f\"\\nRFECV Results per target:\")\n",
    "    if selection_results['rfecv_results']:\n",
    "        for target, results in selection_results['rfecv_results'].items():\n",
    "            cv_scores = results['cv_scores']\n",
    "            try:\n",
    "                if hasattr(cv_scores, '__iter__') and len(cv_scores) > 0:\n",
    "                    best_score = max(cv_scores)\n",
    "                    if isinstance(best_score, (int, float)):\n",
    "                        score_str = f\"{best_score:.4f}\"\n",
    "                    else:\n",
    "                        score_str = str(best_score)\n",
    "                else:\n",
    "                    if isinstance(cv_scores, (int, float)):\n",
    "                        score_str = f\"{cv_scores:.4f}\"\n",
    "                    else:\n",
    "                        score_str = str(cv_scores)\n",
    "            except:\n",
    "                score_str = \"N/A\"\n",
    "            \n",
    "            print(f\"  {target}: {results['optimal_features']} features, \"\n",
    "                  f\"CV score: {score_str}\")\n",
    "    else:\n",
    "        print(\"  No RFECV results available (fallback method used)\")\n",
    "    \n",
    "    print(f\"\\nFinal selection summary:\")\n",
    "    print(f\"  Must-keep features: {len(selection_results['must_keep_cols'])}\")\n",
    "    print(f\"  Available for selection: {len(selection_results['available_features'])}\")\n",
    "    print(f\"  Target additional: {selection_results['target_additional_features']}\")\n",
    "    print(f\"  Final selected: {len(selection_results['final_selected_features'])}\")\n",
    "    \n",
    "    if selection_results['categorical_encoders']:\n",
    "        print(f\"\\nCategorical features encoded: {list(selection_results['categorical_encoders'].keys())}\")\n",
    "\n",
    "# Additional utility function to handle deprecated pandas methods\n",
    "def safe_fillna(df, method='ffill'):\n",
    "    \"\"\"\n",
    "    Safe fillna method that handles deprecated method parameter\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try new approach first (pandas >= 2.0)\n",
    "        if method == 'ffill':\n",
    "            return df.ffill().bfill()\n",
    "        elif method == 'bfill':\n",
    "            return df.bfill().ffill()\n",
    "        else:\n",
    "            return df.fillna(method=method)\n",
    "    except TypeError:\n",
    "        # Fallback to old method for older pandas versions\n",
    "        return df.fillna(method=method)\n",
    "\n",
    "# Updated version that handles pandas deprecation warnings\n",
    "def feature_selection_pipeline_v2(train_df, test_df, n_features_target=35):\n",
    "    \"\"\"\n",
    "    Updated feature selection pipeline that handles pandas deprecation warnings\n",
    "    \"\"\"\n",
    "    # Replace all fillna(method='...') calls with safe_fillna\n",
    "    # This is a wrapper that can be used if you encounter pandas deprecation warnings\n",
    "    return feature_selection_pipeline(train_df, test_df, n_features_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eng = pd.read_csv('../data/fix/feature_engineered_recession_train.csv')\n",
    "test_eng = pd.read_csv('../data/fix/feature_engineered_recession_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE SELECTION PIPELINE - RFECV\n",
      "======================================================================\n",
      "Dataset shape: Train (635, 175), Test (65, 175)\n",
      "Must-keep columns: 17 (['date', 'recession_probability', '1_month_recession_probability', '3_month_recession_probability', '6_month_recession_probability', '1_year_rate', '3_months_rate', '6_months_rate', 'CPI', 'INDPRO', '10_year_rate', 'share_price', 'unemployment_rate', 'PPI', 'OECD_CLI_index', 'CSI_index', 'gdp_per_capita'])\n",
      "Available features for selection: 158\n",
      "Target additional features: 18\n",
      "Actual additional features to select: 18\n",
      "Final target columns: 35\n",
      "\n",
      "Available features for selection:\n",
      "   1. Business_Quarter\n",
      "   2. Month\n",
      "   3. Country\n",
      "   4. 1_year_rate_trend\n",
      "   5. 1_year_rate_seasonal\n",
      "   6. 1_year_rate_residual\n",
      "   7. 3_months_rate_trend\n",
      "   8. 3_months_rate_seasonal\n",
      "   9. 3_months_rate_residual\n",
      "  10. 6_months_rate_trend\n",
      "  11. 6_months_rate_seasonal\n",
      "  12. 6_months_rate_residual\n",
      "  13. CPI_trend\n",
      "  14. CPI_seasonal\n",
      "  15. CPI_residual\n",
      "  16. INDPRO_trend\n",
      "  17. INDPRO_seasonal\n",
      "  18. INDPRO_residual\n",
      "  19. 10_year_rate_trend\n",
      "  20. 10_year_rate_seasonal\n",
      "  ... and 138 more features\n",
      "\n",
      "==================================================\n",
      "RECURSIVE FEATURE ELIMINATION WITH CV\n",
      "==================================================\n",
      "\n",
      "Preparing data for RFECV...\n",
      "Missing values in available features: 0\n",
      "Categorical columns found: ['Business_Quarter', 'Month', 'Country']\n",
      "After preprocessing: 0 missing values\n",
      "RFECV Configuration:\n",
      "  CV folds: 3\n",
      "  RandomForest params: {'n_estimators': 50, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 5, 'random_state': 42, 'n_jobs': -1}\n",
      "  Target additional features: 18\n",
      "\n",
      "[1/12] Processing target: 1_year_rate\n",
      "  Valid samples: 556\n",
      "  Running RFECV...\n",
      "  Optimal features: 5\n",
      "  Best CV score: std_test_score\n",
      "  Selected features: 5\n",
      "\n",
      "[2/12] Processing target: 3_months_rate\n",
      "  Valid samples: 635\n",
      "  Running RFECV...\n",
      "  Optimal features: 5\n",
      "  Best CV score: std_test_score\n",
      "  Selected features: 5\n",
      "\n",
      "[3/12] Processing target: 6_months_rate\n",
      "  Valid samples: 635\n",
      "  Running RFECV...\n",
      "  Optimal features: 6\n",
      "  Best CV score: std_test_score\n",
      "  Selected features: 6\n",
      "\n",
      "[4/12] Processing target: CPI\n",
      "  Valid samples: 635\n",
      "  Running RFECV...\n",
      "  Optimal features: 7\n",
      "  Best CV score: std_test_score\n",
      "  Selected features: 7\n",
      "\n",
      "[5/12] Processing target: INDPRO\n",
      "  Valid samples: 635\n",
      "  Running RFECV...\n",
      "  Optimal features: 23\n",
      "  Best CV score: std_test_score\n",
      "  Selected features: 23\n",
      "\n",
      "[6/12] Processing target: 10_year_rate\n",
      "  Valid samples: 635\n",
      "  Running RFECV...\n",
      "  Optimal features: 9\n",
      "  Best CV score: std_test_score\n",
      "  Selected features: 9\n",
      "\n",
      "[7/12] Processing target: share_price\n",
      "  Valid samples: 635\n",
      "  Running RFECV...\n",
      "  Optimal features: 13\n",
      "  Best CV score: std_test_score\n",
      "  Selected features: 13\n",
      "\n",
      "[8/12] Processing target: unemployment_rate\n",
      "  Valid samples: 635\n",
      "  Running RFECV...\n",
      "  Optimal features: 5\n",
      "  Best CV score: std_test_score\n",
      "  Selected features: 5\n",
      "\n",
      "[9/12] Processing target: PPI\n",
      "  Valid samples: 635\n",
      "  Running RFECV...\n",
      "  Optimal features: 13\n",
      "  Best CV score: std_test_score\n",
      "  Selected features: 13\n",
      "\n",
      "[10/12] Processing target: OECD_CLI_index\n",
      "  Valid samples: 635\n",
      "  Running RFECV...\n",
      "  Optimal features: 5\n",
      "  Best CV score: std_test_score\n",
      "  Selected features: 5\n",
      "\n",
      "[11/12] Processing target: CSI_index\n",
      "  Valid samples: 635\n",
      "  Running RFECV...\n",
      "  Optimal features: 5\n",
      "  Best CV score: std_test_score\n",
      "  Selected features: 5\n",
      "\n",
      "[12/12] Processing target: gdp_per_capita\n",
      "  Valid samples: 635\n",
      "  Running RFECV...\n",
      "  Optimal features: 102\n",
      "  Best CV score: std_test_score\n",
      "  Selected features: 102\n",
      "\n",
      "==================================================\n",
      "AGGREGATING FEATURE SELECTION RESULTS\n",
      "==================================================\n",
      "Processed 12 target variables successfully\n",
      "Selected top 18 features:\n",
      "   1. CSI_index_trend\n",
      "      Importance: 0.6893, Frequency: 0.58, Combined: 0.6575\n",
      "   2. 10_year_rate_trend\n",
      "      Importance: 0.5959, Frequency: 0.50, Combined: 0.5671\n",
      "   3. 3_months_rate_trend\n",
      "      Importance: 0.5864, Frequency: 0.50, Combined: 0.5605\n",
      "   4. 1_year_rate_trend\n",
      "      Importance: 0.5622, Frequency: 0.50, Combined: 0.5435\n",
      "   5. unemployment_rate_trend\n",
      "      Importance: 0.5615, Frequency: 0.50, Combined: 0.5431\n",
      "   6. 6_months_rate_trend\n",
      "      Importance: 0.5501, Frequency: 0.50, Combined: 0.5351\n",
      "   7. PPI_trend\n",
      "      Importance: 0.5340, Frequency: 0.50, Combined: 0.5238\n",
      "   8. CPI_trend\n",
      "      Importance: 0.5226, Frequency: 0.50, Combined: 0.5158\n",
      "   9. gdp_per_capita_trend\n",
      "      Importance: 0.5035, Frequency: 0.42, Combined: 0.4774\n",
      "  10. gdp_per_capita_residual\n",
      "      Importance: 0.5022, Frequency: 0.42, Combined: 0.4765\n",
      "  11. OECD_CLI_index_trend\n",
      "      Importance: 0.4976, Frequency: 0.42, Combined: 0.4733\n",
      "  12. OECD_CLI_index_residual\n",
      "      Importance: 0.4750, Frequency: 0.42, Combined: 0.4575\n",
      "  13. 3_months_rate_residual\n",
      "      Importance: 0.4577, Frequency: 0.42, Combined: 0.4454\n",
      "  14. INDPRO_trend\n",
      "      Importance: 0.4523, Frequency: 0.42, Combined: 0.4416\n",
      "  15. share_price_trend\n",
      "      Importance: 0.4483, Frequency: 0.42, Combined: 0.4388\n",
      "  ... and 3 more features\n",
      "\n",
      "==================================================\n",
      "FINAL FEATURE SELECTION\n",
      "==================================================\n",
      "Final feature set (35 features):\n",
      "  Must-keep: 17 features\n",
      "  Selected: 18 features\n",
      "\n",
      "Feature breakdown:\n",
      "  Mandatory: 1 - ['date']\n",
      "  Recession: 4 - ['recession_probability', '1_month_recession_probability', '3_month_recession_probability', '6_month_recession_probability']\n",
      "  Targets: 12 - ['1_year_rate', '3_months_rate', '6_months_rate', 'CPI', 'INDPRO', '10_year_rate', 'share_price', 'unemployment_rate', 'PPI', 'OECD_CLI_index', 'CSI_index', 'gdp_per_capita']\n",
      "  Selected: 18\n",
      "    Top selected features:\n",
      "       1. CSI_index_trend\n",
      "       2. 10_year_rate_trend\n",
      "       3. 3_months_rate_trend\n",
      "       4. 1_year_rate_trend\n",
      "       5. unemployment_rate_trend\n",
      "       6. 6_months_rate_trend\n",
      "       7. PPI_trend\n",
      "       8. CPI_trend\n",
      "       9. gdp_per_capita_trend\n",
      "      10. gdp_per_capita_residual\n",
      "      ... and 8 more\n",
      "\n",
      "Applying categorical encoding to test data...\n",
      "\n",
      "Final dataset shapes:\n",
      "  Training: (635, 35)\n",
      "  Test: (65, 35)\n",
      "\n",
      "Data quality after selection:\n",
      "  Training missing values: 87\n",
      "  Test missing values: 11\n",
      "\n",
      "======================================================================\n",
      "FEATURE SELECTION COMPLETE\n",
      "======================================================================\n",
      "============================================================\n",
      "FEATURE SELECTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Top 15 features by combined score:\n",
      " 1. CSI_index_trend                Score: 0.6575 (Imp: 0.689, Freq: 0.58)\n",
      " 2. 10_year_rate_trend             Score: 0.5671 (Imp: 0.596, Freq: 0.50)\n",
      " 3. 3_months_rate_trend            Score: 0.5605 (Imp: 0.586, Freq: 0.50)\n",
      " 4. 1_year_rate_trend              Score: 0.5435 (Imp: 0.562, Freq: 0.50)\n",
      " 5. unemployment_rate_trend        Score: 0.5431 (Imp: 0.562, Freq: 0.50)\n",
      " 6. 6_months_rate_trend            Score: 0.5351 (Imp: 0.550, Freq: 0.50)\n",
      " 7. PPI_trend                      Score: 0.5238 (Imp: 0.534, Freq: 0.50)\n",
      " 8. CPI_trend                      Score: 0.5158 (Imp: 0.523, Freq: 0.50)\n",
      " 9. gdp_per_capita_trend           Score: 0.4774 (Imp: 0.503, Freq: 0.42)\n",
      "10. gdp_per_capita_residual        Score: 0.4765 (Imp: 0.502, Freq: 0.42)\n",
      "11. OECD_CLI_index_trend           Score: 0.4733 (Imp: 0.498, Freq: 0.42)\n",
      "12. OECD_CLI_index_residual        Score: 0.4575 (Imp: 0.475, Freq: 0.42)\n",
      "13. 3_months_rate_residual         Score: 0.4454 (Imp: 0.458, Freq: 0.42)\n",
      "14. INDPRO_trend                   Score: 0.4416 (Imp: 0.452, Freq: 0.42)\n",
      "15. share_price_trend              Score: 0.4388 (Imp: 0.448, Freq: 0.42)\n",
      "\n",
      "Selection frequency distribution:\n",
      "  Selected by 1/12 targets: 1 features\n",
      "  Selected by 0/12 targets: 7 features\n",
      "  Selected by 0/12 targets: 7 features\n",
      "  Selected by 0/12 targets: 6 features\n",
      "  Selected by 0/12 targets: 2 features\n",
      "  Selected by 0/12 targets: 5 features\n",
      "  Selected by 0/12 targets: 74 features\n",
      "\n",
      "RFECV Results per target:\n",
      "  1_year_rate: 5 features, CV score: std_test_score\n",
      "  3_months_rate: 5 features, CV score: std_test_score\n",
      "  6_months_rate: 6 features, CV score: std_test_score\n",
      "  CPI: 7 features, CV score: std_test_score\n",
      "  INDPRO: 23 features, CV score: std_test_score\n",
      "  10_year_rate: 9 features, CV score: std_test_score\n",
      "  share_price: 13 features, CV score: std_test_score\n",
      "  unemployment_rate: 5 features, CV score: std_test_score\n",
      "  PPI: 13 features, CV score: std_test_score\n",
      "  OECD_CLI_index: 5 features, CV score: std_test_score\n",
      "  CSI_index: 5 features, CV score: std_test_score\n",
      "  gdp_per_capita: 102 features, CV score: std_test_score\n",
      "\n",
      "Final selection summary:\n",
      "  Must-keep features: 17\n",
      "  Available for selection: 158\n",
      "  Target additional: 18\n",
      "  Final selected: 35\n",
      "\n",
      "Categorical features encoded: ['Business_Quarter', 'Month', 'Country']\n"
     ]
    }
   ],
   "source": [
    "# After running your STL and Fourier pipelines\n",
    "train_selected, test_selected, results = feature_selection_pipeline(\n",
    "    train_eng, test_eng, n_features_target=35\n",
    ")\n",
    "\n",
    "# Analyze the selection results\n",
    "analyze_selection_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                              object\n",
       "recession_probability            float64\n",
       "1_month_recession_probability    float64\n",
       "3_month_recession_probability    float64\n",
       "6_month_recession_probability    float64\n",
       "1_year_rate                      float64\n",
       "3_months_rate                    float64\n",
       "6_months_rate                    float64\n",
       "CPI                              float64\n",
       "INDPRO                           float64\n",
       "10_year_rate                     float64\n",
       "share_price                      float64\n",
       "unemployment_rate                float64\n",
       "PPI                              float64\n",
       "OECD_CLI_index                   float64\n",
       "CSI_index                        float64\n",
       "gdp_per_capita                   float64\n",
       "CSI_index_trend                  float64\n",
       "10_year_rate_trend               float64\n",
       "3_months_rate_trend              float64\n",
       "1_year_rate_trend                float64\n",
       "unemployment_rate_trend          float64\n",
       "6_months_rate_trend              float64\n",
       "PPI_trend                        float64\n",
       "CPI_trend                        float64\n",
       "gdp_per_capita_trend             float64\n",
       "gdp_per_capita_residual          float64\n",
       "OECD_CLI_index_trend             float64\n",
       "OECD_CLI_index_residual          float64\n",
       "3_months_rate_residual           float64\n",
       "INDPRO_trend                     float64\n",
       "share_price_trend                float64\n",
       "6_months_rate_residual           float64\n",
       "1_year_rate_residual             float64\n",
       "10_year_rate_residual            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_selected.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selected.to_csv('../data/fix/feature_selected_recession_train.csv', index=False)\n",
    "test_selected.to_csv('../data/fix/feature_selected_recession_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Columns only in train_final:\n",
      "None\n",
      "\n",
      "🔹 Columns only in test_final:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Columns that are in train_final but not in test_final\n",
    "train_only = set(train_selected.columns) - set(test_selected.columns)\n",
    "\n",
    "# Columns that are in test_final but not in train_final\n",
    "test_only = set(test_selected.columns) - set(train_selected.columns)\n",
    "\n",
    "print(\"🔹 Columns only in train_final:\")\n",
    "print(train_only if train_only else \"None\")\n",
    "\n",
    "print(\"\\n🔹 Columns only in test_final:\")\n",
    "print(test_only if test_only else \"None\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
