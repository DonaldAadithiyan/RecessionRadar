{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f0c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Financial indicators to forecast\n",
    "financial_indicators = [\n",
    "    '1_year_rate', '3_months_rate', '6_months_rate', 'CPI', 'INDPRO',\n",
    "    '10_year_rate', 'share_price', 'unemployment_rate', 'PPI',\n",
    "    'OECD_CLI_index', 'CSI_index', 'gdp_per_capita'\n",
    "]\n",
    "\n",
    "# Recession probability target columns to exclude from features\n",
    "recession_targets = [\n",
    "    'recession_probability', '1_month_recession_probability',\n",
    "    '3_month_recession_probability', '6_month_recession_probability'\n",
    "]\n",
    "\n",
    "def prepare_existing_features(train_df, test_df, target_indicator):\n",
    "    \"\"\"Prepare existing features from train and test dataframes, excluding recession targets\"\"\"\n",
    "    print(f\"\\nPreparing existing features for {target_indicator}...\")\n",
    "    \n",
    "    # Define features to exclude (date, target, and recession targets)\n",
    "    features_to_exclude = ['date'] + recession_targets + [target_indicator]\n",
    "    \n",
    "    # Get available features from training data\n",
    "    available_features = [c for c in train_df.columns if c not in features_to_exclude]\n",
    "    \n",
    "    print(f\"Available existing features: {len(available_features)}\")\n",
    "    print(f\"Features: {available_features[:10]}...\" if len(available_features) > 10 else f\"Features: {available_features}\")\n",
    "    \n",
    "    # Ensure test data has the same features\n",
    "    common_features = [f for f in available_features if f in test_df.columns]\n",
    "    \n",
    "    if len(common_features) != len(available_features):\n",
    "        missing_in_test = set(available_features) - set(common_features)\n",
    "        print(f\"Warning: {len(missing_in_test)} features missing in test data: {missing_in_test}\")\n",
    "    \n",
    "    train_features = train_df[common_features].copy()\n",
    "    test_features = test_df[common_features].copy()\n",
    "    \n",
    "    return train_features, test_features, common_features\n",
    "\n",
    "def clean_existing_features(train_features, test_features, feature_cols):\n",
    "    \"\"\"Clean existing feature data\"\"\"\n",
    "    print(f\"Cleaning existing feature data...\")\n",
    "    print(f\"  Initial shapes - Train: {train_features.shape}, Test: {test_features.shape}\")\n",
    "    \n",
    "    # Handle infinite values and NaNs\n",
    "    for col in feature_cols:\n",
    "        if col in train_features.columns:\n",
    "            # Clean training features\n",
    "            train_features[col] = train_features[col].replace([np.inf, -np.inf], np.nan)\n",
    "            train_features[col] = train_features[col].fillna(method='ffill').fillna(method='bfill')\n",
    "            if train_features[col].isna().any():\n",
    "                train_features[col] = train_features[col].fillna(train_features[col].median())\n",
    "            if train_features[col].isna().any():\n",
    "                train_features[col] = train_features[col].fillna(0)\n",
    "        \n",
    "        if col in test_features.columns:\n",
    "            # Clean test features\n",
    "            test_features[col] = test_features[col].replace([np.inf, -np.inf], np.nan)\n",
    "            test_features[col] = test_features[col].fillna(method='ffill').fillna(method='bfill')\n",
    "            if test_features[col].isna().any():\n",
    "                # Use training median for consistency\n",
    "                fill_value = train_features[col].median() if col in train_features.columns else 0\n",
    "                test_features[col] = test_features[col].fillna(fill_value)\n",
    "            if test_features[col].isna().any():\n",
    "                test_features[col] = test_features[col].fillna(0)\n",
    "    \n",
    "    # Remove columns with no variation in training data\n",
    "    varying_cols = []\n",
    "    for col in feature_cols:\n",
    "        if col in train_features.columns and train_features[col].nunique() > 1:\n",
    "            varying_cols.append(col)\n",
    "    \n",
    "    # Keep only varying columns in both datasets\n",
    "    train_features_clean = train_features[varying_cols]\n",
    "    test_features_clean = test_features[[c for c in varying_cols if c in test_features.columns]]\n",
    "    \n",
    "    print(f\"  Final shapes - Train: {train_features_clean.shape}, Test: {test_features_clean.shape}\")\n",
    "    print(f\"  Features with variation: {len(varying_cols)}\")\n",
    "    print(f\"  Remaining missing values - Train: {train_features_clean.isnull().sum().sum()}, Test: {test_features_clean.isnull().sum().sum()}\")\n",
    "    \n",
    "    return train_features_clean, test_features_clean, varying_cols\n",
    "\n",
    "def make_xgb_objective(X_train, y_train, X_val, y_val, random_state=42):\n",
    "    \"\"\"Create Optuna objective function for XGBoost hyperparameter tuning\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 2000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 8),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 10.0),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.5, 5.0),\n",
    "            \"random_state\": random_state,\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"tree_method\": \"hist\",\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=False\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        return rmse\n",
    "    return objective\n",
    "\n",
    "def time_val_split(X, y, val_ratio=0.2):\n",
    "    \"\"\"Keep order; last portion as validation.\"\"\"\n",
    "    n = len(X)\n",
    "    cut = int(np.floor(n * (1 - val_ratio)))\n",
    "    return X[:cut], X[cut:], y[:cut], y[cut:]\n",
    "\n",
    "def forecast_indicator_with_xgb(train_df, test_df, indicator, n_trials=50):\n",
    "    \"\"\"Forecast indicator using XGBoost with existing features only\"\"\"\n",
    "    print(f\"\\n{'='*60}\\nFORECASTING: {indicator}\\n{'='*60}\")\n",
    "    \n",
    "    if indicator not in train_df.columns or indicator not in test_df.columns:\n",
    "        print(f\"ERROR: {indicator} not in datasets\")\n",
    "        return None\n",
    "    \n",
    "    # Get target series\n",
    "    train_series = train_df[indicator].dropna()\n",
    "    test_series = test_df[indicator].dropna()\n",
    "    print(f\"Series lengths - Train: {len(train_series)}, Test: {len(test_series)}\")\n",
    "    \n",
    "    if len(train_series) < 10:\n",
    "        print(f\"ERROR: Insufficient training data for {indicator}\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare existing features\n",
    "    try:\n",
    "        train_features, test_features, available_features = prepare_existing_features(train_df, test_df, indicator)\n",
    "        train_features_clean, test_features_clean, feature_cols = clean_existing_features(train_features, test_features, available_features)\n",
    "        \n",
    "        if len(feature_cols) == 0:\n",
    "            print(f\"WARNING: No features available for {indicator}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR preparing features: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Align features with target series (handle any index mismatches)\n",
    "    train_features_aligned = train_features_clean.loc[train_series.index]\n",
    "    test_features_aligned = test_features_clean.loc[test_series.index]\n",
    "    \n",
    "    # Remove any remaining NaN rows\n",
    "    train_mask = ~(train_series.isna() | train_features_aligned.isna().any(axis=1))\n",
    "    test_mask = ~(test_series.isna() | test_features_aligned.isna().any(axis=1))\n",
    "    \n",
    "    X_train_full = train_features_aligned.loc[train_mask].values\n",
    "    y_train_full = train_series.loc[train_mask].values\n",
    "    X_test = test_features_aligned.loc[test_mask].values\n",
    "    y_test = test_series.loc[test_mask].values\n",
    "    \n",
    "    if len(X_train_full) < 10:\n",
    "        print(f\"ERROR: Insufficient clean training data for {indicator}: {len(X_train_full)} rows\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Clean data shapes - Train: {X_train_full.shape}, Test: {X_test.shape}\")\n",
    "    print(f\"Using {len(feature_cols)} features\")\n",
    "    \n",
    "    # Split training data for hyperparameter tuning\n",
    "    X_train, X_val, y_train, y_val = time_val_split(X_train_full, y_train_full)\n",
    "    \n",
    "    # Hyperparameter tuning with Optuna\n",
    "    print(f\"\\nRunning XGBoost hyperparameter optimization...\")\n",
    "    print(f\"Training shape: {X_train.shape}, Validation shape: {X_val.shape}\")\n",
    "    \n",
    "    try:\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(\n",
    "            make_xgb_objective(X_train, y_train, X_val, y_val, random_state=42), \n",
    "            n_trials=n_trials, \n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best validation RMSE: {study.best_value:.4f}\")\n",
    "        \n",
    "        # Train final model on full training data with validation for early stopping\n",
    "        X_train_final, X_val_final, y_train_final, y_val_final = time_val_split(X_train_full, y_train_full)\n",
    "        \n",
    "        final_model = XGBRegressor(\n",
    "            **{k: v for k, v in best_params.items() if k != \"random_state\"},\n",
    "            random_state=42,\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\"\n",
    "        )\n",
    "        \n",
    "        final_model.fit(\n",
    "            X_train_final, y_train_final,\n",
    "            eval_set=[(X_val_final, y_val_final)],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Make predictions on test data\n",
    "        forecast = final_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test, forecast)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, forecast))\n",
    "        \n",
    "        # Calculate MAPE with mask for non-zero values\n",
    "        mask = y_test != 0\n",
    "        mape = np.mean(np.abs((y_test[mask] - forecast[mask]) / y_test[mask])) * 100 if mask.any() else np.inf\n",
    "        \n",
    "        print(f\"\\nAccuracy Metrics:\\n  MAE: {mae:.4f}\\n  RMSE: {rmse:.4f}\\n  MAPE: {mape:.2f}%\")\n",
    "        \n",
    "        # Save model\n",
    "        os.makedirs(\"xgb_models\", exist_ok=True)\n",
    "        with open(f\"xgb_models/{indicator}_xgb_model.pkl\", \"wb\") as f:\n",
    "            pickle.dump(final_model, f)\n",
    "        print(f\"Model saved to xgb_models/{indicator}_xgb_model.pkl\")\n",
    "        \n",
    "        return {\n",
    "            'model': final_model,\n",
    "            'best_params': best_params,\n",
    "            'best_val_rmse': study.best_value,\n",
    "            'forecast': forecast,\n",
    "            'actual': y_test,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape,\n",
    "            'feature_cols': feature_cols,\n",
    "            'train_length': len(y_train_full),\n",
    "            'test_length': len(y_test),\n",
    "            'train_dates': train_df.loc[train_series.loc[train_mask].index, 'date'].values,\n",
    "            'test_dates': test_df.loc[test_series.loc[test_mask].index, 'date'].values\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR training XGBoost for {indicator}: {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_xgb_forecast_results(results, indicator, train_df, test_df):\n",
    "    \"\"\"Plot XGBoost forecast results similar to ARIMA style\"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "        \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot training data (last portion for visibility)\n",
    "    train_series = train_df[indicator].dropna()\n",
    "    train_plot = train_series.tail(min(50, len(train_series)))\n",
    "    train_dates = train_df.loc[train_plot.index, 'date']\n",
    "    \n",
    "    plt.plot(train_dates, train_plot.values, label='Training', color='blue', alpha=0.7)\n",
    "    \n",
    "    # Plot actual and forecast\n",
    "    actual = results['actual']\n",
    "    forecast = results['forecast']\n",
    "    test_dates = results['test_dates']\n",
    "    \n",
    "    plt.plot(test_dates, actual, label='Actual', color='green', linewidth=2, marker='o', markersize=4)\n",
    "    plt.plot(test_dates, forecast, label='Forecast', color='red', linewidth=2, marker='s', markersize=4)\n",
    "    \n",
    "    # Add title with metrics\n",
    "    plt.title(f'{indicator} - XGBoost Forecast\\n'\n",
    "              f'MAE: {results[\"mae\"]:.4f}, RMSE: {results[\"rmse\"]:.4f}, MAPE: {results[\"mape\"]:.2f}%',\n",
    "              fontsize=14)\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add vertical line to separate training and test\n",
    "    if len(test_dates) > 0:\n",
    "        plt.axvline(x=test_dates[0], color='gray', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def run_xgb_forecasting_existing_features(train_df, test_df, financial_indicators=None, n_trials=40, plot_results=True):\n",
    "    \"\"\"Run XGBoost forecasting pipeline using existing features only\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"XGBoost TIME SERIES FORECASTING - EXISTING FEATURES ONLY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if financial_indicators is None:\n",
    "        financial_indicators = [\n",
    "            '1_year_rate', '3_months_rate', '6_months_rate', 'CPI', 'INDPRO',\n",
    "            '10_year_rate', 'share_price', 'unemployment_rate', 'PPI',\n",
    "            'OECD_CLI_index', 'CSI_index', 'gdp_per_capita'\n",
    "        ]\n",
    "    \n",
    "    train_work = train_df.copy()\n",
    "    test_work = test_df.copy()\n",
    "    train_work['date'] = pd.to_datetime(train_work['date'])\n",
    "    test_work['date'] = pd.to_datetime(test_work['date'])\n",
    "    \n",
    "    # Check available indicators\n",
    "    available = [i for i in financial_indicators if i in train_work.columns and i in test_work.columns]\n",
    "    print(f\"Indicators to forecast: {available}\")\n",
    "    print(f\"Excluding recession features: {recession_targets}\")\n",
    "    \n",
    "    all_results = {}\n",
    "    forecasted = pd.DataFrame({'date': test_work['date'].copy()})\n",
    "    \n",
    "    for ind in available:\n",
    "        res = forecast_indicator_with_xgb(\n",
    "            train_work, test_work, ind, \n",
    "            n_trials=n_trials\n",
    "        )\n",
    "        \n",
    "        if res:\n",
    "            all_results[ind] = res\n",
    "            \n",
    "            # Add results to forecasted dataframe\n",
    "            steps = len(res['forecast'])\n",
    "            forecasted[f'{ind}_forecast'] = np.nan\n",
    "            forecasted[f'{ind}_actual'] = np.nan\n",
    "            \n",
    "            # Find matching indices for test dates\n",
    "            test_dates_df = pd.DataFrame({'date': pd.to_datetime(res['test_dates'])})\n",
    "            merged = forecasted.merge(test_dates_df.reset_index(), on='date', how='left')\n",
    "            valid_indices = merged.dropna()['index'].astype(int).values[:steps]\n",
    "            \n",
    "            if len(valid_indices) == len(res['forecast']):\n",
    "                forecasted.loc[valid_indices, f'{ind}_forecast'] = res['forecast']\n",
    "                forecasted.loc[valid_indices, f'{ind}_actual'] = res['actual']\n",
    "            \n",
    "            if plot_results:\n",
    "                plot_xgb_forecast_results(res, ind, train_work, test_work)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FORECASTING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Successfully forecasted {len(all_results)} indicators\")\n",
    "    \n",
    "    # Print summary metrics\n",
    "    if all_results:\n",
    "        print(f\"\\nSUMMARY METRICS:\")\n",
    "        for ind, res in all_results.items():\n",
    "            print(f\"{ind:20s} - MAE: {res['mae']:.4f}, RMSE: {res['rmse']:.4f}, MAPE: {res['mape']:.2f}%\")\n",
    "    \n",
    "    return all_results, forecasted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86703e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Load your data with train/test split\n",
    "train_df = pd.read_csv('')\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "\n",
    "results, forecasts = run_xgb_forecasting_existing_features(\n",
    "    train_df, test_df,\n",
    "    financial_indicators=financial_indicators,\n",
    "    n_trials=40,\n",
    "    plot_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17abbc53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda3ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
