{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 635 observations (1967-02-01 00:00:00 → 2019-12-01 00:00:00)\n",
      "Test data: 65 observations (2020-01-01 00:00:00 → 2025-05-01 00:00:00)\n",
      "\n",
      "Train head:\n",
      "        date  recession_probability  1_month_recession_probability  \\\n",
      "0 1967-02-01                    NaN                            NaN   \n",
      "1 1967-03-01                    NaN                            NaN   \n",
      "2 1967-04-01                    NaN                            NaN   \n",
      "3 1967-05-01                    NaN                            0.8   \n",
      "4 1967-06-01                    0.8                            0.5   \n",
      "\n",
      "   3_month_recession_probability  6_month_recession_probability  1_year_rate  \\\n",
      "0                            NaN                           0.00     4.574444   \n",
      "1                           0.80                           0.26     4.185000   \n",
      "2                           0.50                           0.08     3.898500   \n",
      "3                           0.00                           0.00     3.885000   \n",
      "4                           0.26                           0.00     4.164545   \n",
      "\n",
      "   3_months_rate  6_months_rate   CPI   INDPRO  10_year_rate  share_price  \\\n",
      "0       4.561111       4.590556  33.0  34.7650          4.63     4.733288   \n",
      "1       4.259091       4.221818  33.0  34.5691          4.54     4.854181   \n",
      "2       3.837500       3.894500  33.1  34.8951          4.59     4.943822   \n",
      "3       3.603182       3.799545  33.1  34.5907          4.85     5.050295   \n",
      "4       3.535909       3.889545  33.3  34.5863          5.02     5.005238   \n",
      "\n",
      "   unemployment_rate   PPI  OECD_CLI_index  CSI_index Business_Quarter  \\\n",
      "0                3.8  27.2       98.590266       94.1               Q1   \n",
      "1                3.8  27.2       98.606358       94.1               Q1   \n",
      "2                3.8  27.2       98.745387       94.1               Q2   \n",
      "3                3.8  27.2       98.993997       95.9               Q2   \n",
      "4                3.9  27.2       99.304794       95.9               Q2   \n",
      "\n",
      "      Month Country  gdp_per_capita  \n",
      "0  February     USA    24586.666667  \n",
      "1     March     USA    24571.333333  \n",
      "2     April     USA    24556.000000  \n",
      "3       May     USA    24610.333333  \n",
      "4      June     USA    24664.666667  \n",
      "\n",
      "Test head:\n",
      "          date  recession_probability  1_month_recession_probability  \\\n",
      "635 2020-01-01                   0.26                           0.10   \n",
      "636 2020-02-01                   0.10                         100.00   \n",
      "637 2020-03-01                 100.00                         100.00   \n",
      "638 2020-04-01                 100.00                           0.08   \n",
      "639 2020-05-01                   0.08                           0.12   \n",
      "\n",
      "     3_month_recession_probability  6_month_recession_probability  \\\n",
      "635                         100.00                           0.02   \n",
      "636                           0.08                           0.00   \n",
      "637                           0.12                           0.00   \n",
      "638                           0.02                           0.04   \n",
      "639                           0.00                           0.22   \n",
      "\n",
      "     1_year_rate  3_months_rate  6_months_rate      CPI    INDPRO  \\\n",
      "635     1.493333       1.522381       1.523333  259.127  101.3372   \n",
      "636     1.372632       1.515263       1.471579  259.250  101.6718   \n",
      "637     0.322273       0.286364       0.292727  258.076   97.6060   \n",
      "638     0.183333       0.138095       0.169048  256.032   84.6812   \n",
      "639     0.161500       0.127500       0.153500  255.802   86.0108   \n",
      "\n",
      "     10_year_rate  share_price  unemployment_rate    PPI  OECD_CLI_index  \\\n",
      "635          1.76     130.9052                3.6  334.7       99.161072   \n",
      "636          1.50     128.6936                3.5  336.9       99.115753   \n",
      "637          0.87     100.4580                4.4  338.9       97.698344   \n",
      "638          0.66     101.8041               14.8  334.7       92.843120   \n",
      "639          0.67     105.6555               13.2  330.6       94.399460   \n",
      "\n",
      "     CSI_index Business_Quarter     Month Country  gdp_per_capita  \n",
      "635       99.8               Q1   January     USA    62415.000000  \n",
      "636      101.0               Q1  February     USA    60756.666667  \n",
      "637       89.1               Q1     March     USA    59098.333333  \n",
      "638       71.8               Q2     April     USA    57440.000000  \n",
      "639       72.3               Q2       May     USA    58928.666667  \n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ Define the split function\n",
    "def prepare_data_split(df, date_col='date', split_year=2020):\n",
    "    # Ensure the date column is in datetime format\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Sort by date\n",
    "    df_sorted = df.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "    # Split\n",
    "    train_data = df_sorted[df_sorted[date_col].dt.year < split_year].copy()\n",
    "    test_data = df_sorted[df_sorted[date_col].dt.year >= split_year].copy()\n",
    "\n",
    "    # Print info\n",
    "    print(f\"Training data: {len(train_data)} observations \"\n",
    "          f\"({train_data[date_col].min()} → {train_data[date_col].max()})\")\n",
    "    print(f\"Test data: {len(test_data)} observations \"\n",
    "          f\"({test_data[date_col].min()} → {test_data[date_col].max()})\")\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "# 2️⃣ Load your CSV\n",
    "# Replace with the actual path to your file\n",
    "df = pd.read_csv(\"../data/combined/recession_probability.csv\")\n",
    "\n",
    "# 3️⃣ Call the function\n",
    "train_df, test_df = prepare_data_split(df, date_col=\"date\", split_year=2020)\n",
    "\n",
    "# 4️⃣ Inspect\n",
    "print(\"\\nTrain head:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest head:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"../data/fix/recession_train.csv\", index=False)\n",
    "test_df.to_csv(\"../data/fix/recession_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'recession_probability', '1_month_recession_probability',\n",
       "       '3_month_recession_probability', '6_month_recession_probability',\n",
       "       '1_year_rate', '3_months_rate', '6_months_rate', 'CPI', 'INDPRO',\n",
       "       '10_year_rate', 'share_price', 'unemployment_rate', 'PPI',\n",
       "       'OECD_CLI_index', 'CSI_index', 'Business_Quarter', 'Month', 'Country',\n",
       "       'gdp_per_capita'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def safe_feature_engineering_pipeline(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Safe feature engineering pipeline that prevents data leakage.\n",
    "    Only uses training data to compute all statistics and parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"SAFE FEATURE ENGINEERING PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Make copies to avoid modifying originals\n",
    "    train_fe = train_df.copy()\n",
    "    test_fe = test_df.copy()\n",
    "    \n",
    "    # Ensure date columns are datetime\n",
    "    train_fe['date'] = pd.to_datetime(train_fe['date'])\n",
    "    test_fe['date'] = pd.to_datetime(test_fe['date'])\n",
    "    \n",
    "    # Sort by date to ensure proper time series order\n",
    "    train_fe = train_fe.sort_values('date').reset_index(drop=True)\n",
    "    test_fe = test_fe.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Training data: {train_fe['date'].min()} to {train_fe['date'].max()}\")\n",
    "    print(f\"Test data: {test_fe['date'].min()} to {test_fe['date'].max()}\")\n",
    "    \n",
    "    # Define feature columns\n",
    "    financial_indicators = [\n",
    "        '1_year_rate', '3_months_rate', '6_months_rate', 'CPI', 'INDPRO', \n",
    "        '10_year_rate', 'share_price', 'unemployment_rate', 'PPI', \n",
    "        'OECD_CLI_index', 'CSI_index', 'gdp_per_capita'\n",
    "    ]\n",
    "    \n",
    "    anomaly_columns = [\n",
    "        'INDPRO', 'CPI', 'unemployment_rate', 'PPI', 'share_price',\n",
    "        '1_year_rate', '3_months_rate', '6_months_rate', '10_year_rate'\n",
    "    ]\n",
    "    \n",
    "    acf_columns = financial_indicators  # Same as financial_indicators\n",
    "    \n",
    "    print(f\"\\nProcessing {len(financial_indicators)} financial indicators...\")\n",
    "    print(f\"Anomaly detection for {len(anomaly_columns)} columns...\")\n",
    "    print(f\"ACF features for {len(acf_columns)} columns...\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. STL DECOMPOSITION (TRAINING DATA ONLY)\n",
    "    # ==========================================\n",
    "    print(\"\\n1. STL Decomposition (Training Data Only)...\")\n",
    "    \n",
    "    stl_params = {}  # Store parameters computed from training\n",
    "    \n",
    "    for col in financial_indicators:\n",
    "        print(f\"   Processing {col}...\")\n",
    "        \n",
    "        if col not in train_fe.columns:\n",
    "            print(f\"   WARNING: {col} not found in training data, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        # Clean training data\n",
    "        train_series = train_fe[col].fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        if train_series.isna().all() or len(train_series.dropna()) < 24:\n",
    "            print(f\"   WARNING: Insufficient data for {col}, skipping STL...\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # STL decomposition on training data\n",
    "            stl = STL(train_series, seasonal=13, period=12)  # Monthly seasonality\n",
    "            decomposition = stl.fit()\n",
    "            \n",
    "            # Extract components for training\n",
    "            train_fe[f'{col}_trend'] = decomposition.trend\n",
    "            train_fe[f'{col}_seasonal'] = decomposition.seasonal  \n",
    "            train_fe[f'{col}_residual'] = decomposition.resid\n",
    "            \n",
    "            # Store parameters for test application\n",
    "            stl_params[col] = {\n",
    "                'trend': decomposition.trend,\n",
    "                'seasonal': decomposition.seasonal,\n",
    "                'residual': decomposition.resid,\n",
    "                'last_trend': decomposition.trend.iloc[-1],\n",
    "                'seasonal_pattern': decomposition.seasonal.iloc[-12:].values,  # Last year pattern\n",
    "                'residual_mean': decomposition.resid.mean(),\n",
    "                'residual_std': decomposition.resid.std()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR in STL for {col}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Apply STL-derived features to test data (using training parameters)\n",
    "    print(\"\\n   Applying STL features to test data...\")\n",
    "    \n",
    "    for col in financial_indicators:\n",
    "        if col not in stl_params or col not in test_fe.columns:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # For test data, we can't do full STL decomposition as it would use future data\n",
    "            # Instead, we estimate components using training patterns\n",
    "            \n",
    "            test_series = test_fe[col].fillna(method='ffill').fillna(method='bfill')\n",
    "            \n",
    "            # Trend: Simple extrapolation from last training trend\n",
    "            last_trend = stl_params[col]['last_trend']\n",
    "            test_fe[f'{col}_trend'] = last_trend  # Constant trend assumption\n",
    "            \n",
    "            # Seasonal: Repeat the seasonal pattern from training\n",
    "            seasonal_pattern = stl_params[col]['seasonal_pattern']\n",
    "            n_test = len(test_fe)\n",
    "            seasonal_test = np.tile(seasonal_pattern, (n_test // 12) + 1)[:n_test]\n",
    "            test_fe[f'{col}_seasonal'] = seasonal_test\n",
    "            \n",
    "            # Residual: Estimate as deviation from trend + seasonal\n",
    "            expected = test_fe[f'{col}_trend'] + test_fe[f'{col}_seasonal']\n",
    "            test_fe[f'{col}_residual'] = test_series - expected\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR applying STL to test for {col}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. ANOMALY DETECTION (TRAINING DATA ONLY)\n",
    "    # ==========================================\n",
    "    print(\"\\n2. Anomaly Detection (Training Data Only)...\")\n",
    "    \n",
    "    anomaly_params = {}\n",
    "    \n",
    "    for col in anomaly_columns:\n",
    "        if f'{col}_residual' not in train_fe.columns:\n",
    "            print(f\"   WARNING: No residual for {col}, skipping anomaly detection...\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"   Processing anomalies for {col}...\")\n",
    "        \n",
    "        # Compute anomaly thresholds from training residuals only\n",
    "        residuals = train_fe[f'{col}_residual'].dropna()\n",
    "        \n",
    "        if len(residuals) < 10:\n",
    "            continue\n",
    "            \n",
    "        mean_resid = residuals.mean()\n",
    "        std_resid = residuals.std()\n",
    "        \n",
    "        if std_resid == 0:\n",
    "            std_resid = 1e-6  # Avoid division by zero\n",
    "            \n",
    "        # Store parameters\n",
    "        anomaly_params[col] = {\n",
    "            'mean': mean_resid,\n",
    "            'std': std_resid,\n",
    "            'lower_threshold': mean_resid - 3 * std_resid,\n",
    "            'upper_threshold': mean_resid + 3 * std_resid\n",
    "        }\n",
    "        \n",
    "        # Apply to training data\n",
    "        train_fe[f'{col}_anomaly'] = (\n",
    "            (train_fe[f'{col}_residual'] < anomaly_params[col]['lower_threshold']) |\n",
    "            (train_fe[f'{col}_residual'] > anomaly_params[col]['upper_threshold'])\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Apply to test data using training thresholds\n",
    "        if f'{col}_residual' in test_fe.columns:\n",
    "            test_fe[f'{col}_anomaly'] = (\n",
    "                (test_fe[f'{col}_residual'] < anomaly_params[col]['lower_threshold']) |\n",
    "                (test_fe[f'{col}_residual'] > anomaly_params[col]['upper_threshold'])\n",
    "            ).astype(int)\n",
    "        \n",
    "        anomaly_count_train = train_fe[f'{col}_anomaly'].sum()\n",
    "        anomaly_count_test = test_fe[f'{col}_anomaly'].sum() if f'{col}_anomaly' in test_fe.columns else 0\n",
    "        \n",
    "        print(f\"   {col}: {anomaly_count_train} anomalies in training, {anomaly_count_test} in test\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 3. ACF FEATURES (TRAINING DATA ONLY)\n",
    "    # ==========================================\n",
    "    print(\"\\n3. ACF Features (Training Data Only)...\")\n",
    "    \n",
    "    acf_params = {}\n",
    "    \n",
    "    def compute_acf_features(series, max_lags=10):\n",
    "        \"\"\"Compute all ACF-based features for a series\"\"\"\n",
    "        if len(series.dropna()) < max_lags + 5:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            clean_series = series.fillna(method='ffill').fillna(method='bfill').dropna()\n",
    "            \n",
    "            if len(clean_series) < max_lags + 5:\n",
    "                return None\n",
    "            \n",
    "            # Original series ACF\n",
    "            acf_original = acf(clean_series, nlags=max_lags, fft=True)\n",
    "            first_acf_original = acf_original[1] if len(acf_original) > 1 else 0\n",
    "            sumsq_acf_original = np.sum(acf_original[1:] ** 2) if len(acf_original) > 1 else 0\n",
    "            \n",
    "            # First differenced series\n",
    "            diff1_series = clean_series.diff().dropna()\n",
    "            if len(diff1_series) >= max_lags + 2:\n",
    "                acf_diff1 = acf(diff1_series, nlags=max_lags, fft=True)\n",
    "                first_acf_diff1 = acf_diff1[1] if len(acf_diff1) > 1 else 0\n",
    "                sumsq_acf_diff1 = np.sum(acf_diff1[1:] ** 2) if len(acf_diff1) > 1 else 0\n",
    "            else:\n",
    "                first_acf_diff1 = 0\n",
    "                sumsq_acf_diff1 = 0\n",
    "            \n",
    "            # Second differenced series  \n",
    "            diff2_series = diff1_series.diff().dropna()\n",
    "            if len(diff2_series) >= max_lags + 2:\n",
    "                acf_diff2 = acf(diff2_series, nlags=max_lags, fft=True)\n",
    "                first_acf_diff2 = acf_diff2[1] if len(acf_diff2) > 1 else 0\n",
    "                sumsq_acf_diff2 = np.sum(acf_diff2[1:] ** 2) if len(acf_diff2) > 1 else 0\n",
    "            else:\n",
    "                first_acf_diff2 = 0\n",
    "                sumsq_acf_diff2 = 0\n",
    "                \n",
    "            # Seasonal ACF (lag 12 for monthly data)\n",
    "            seasonal_lag = min(12, len(acf_original) - 1)\n",
    "            seasonal_acf = acf_original[seasonal_lag] if seasonal_lag > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                'first_acf_original': first_acf_original,\n",
    "                'sumsq_acf_original': sumsq_acf_original,\n",
    "                'first_acf_diff1': first_acf_diff1,\n",
    "                'sumsq_acf_diff1': sumsq_acf_diff1,\n",
    "                'first_acf_diff2': first_acf_diff2,\n",
    "                'sumsq_acf_diff2': sumsq_acf_diff2,\n",
    "                'seasonal_acf': seasonal_acf\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR computing ACF: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    # Compute ACF features for each column using TRAINING DATA ONLY\n",
    "    for col in acf_columns:\n",
    "        if col not in train_fe.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"   Computing ACF features for {col}...\")\n",
    "        \n",
    "        # Compute ACF features from training data\n",
    "        train_series = train_fe[col]\n",
    "        acf_features = compute_acf_features(train_series)\n",
    "        \n",
    "        if acf_features is None:\n",
    "            print(f\"   WARNING: Could not compute ACF features for {col}\")\n",
    "            continue\n",
    "            \n",
    "        # Store parameters for test application\n",
    "        acf_params[col] = acf_features\n",
    "        \n",
    "        # Add ACF features to training data\n",
    "        for feature_name, feature_value in acf_features.items():\n",
    "            train_fe[f'{col}_{feature_name}'] = feature_value\n",
    "        \n",
    "        # For test data: Use the SAME values computed from training\n",
    "        # This assumes ACF properties are stationary\n",
    "        for feature_name, feature_value in acf_features.items():\n",
    "            test_fe[f'{col}_{feature_name}'] = feature_value\n",
    "    \n",
    "    # ==========================================\n",
    "    # 4. SUMMARY AND VALIDATION\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Count new features\n",
    "    original_train_cols = len(train_df.columns)\n",
    "    original_test_cols = len(test_df.columns)\n",
    "    new_train_cols = len(train_fe.columns)\n",
    "    new_test_cols = len(test_fe.columns)\n",
    "    \n",
    "    print(f\"Training data: {original_train_cols} → {new_train_cols} columns (+{new_train_cols - original_train_cols})\")\n",
    "    print(f\"Test data: {original_test_cols} → {new_test_cols} columns (+{new_test_cols - original_test_cols})\")\n",
    "    \n",
    "    # Feature breakdown\n",
    "    stl_features = len([col for col in train_fe.columns if any(suffix in col for suffix in ['_trend', '_seasonal', '_residual'])])\n",
    "    anomaly_features = len([col for col in train_fe.columns if '_anomaly' in col])\n",
    "    acf_features = len([col for col in train_fe.columns if any(acf_type in col for acf_type in ['_first_acf', '_sumsq_acf', '_seasonal_acf'])])\n",
    "    \n",
    "    print(f\"\\nFeature breakdown:\")\n",
    "    print(f\"  STL features: {stl_features}\")\n",
    "    print(f\"  Anomaly features: {anomaly_features}\")  \n",
    "    print(f\"  ACF features: {acf_features}\")\n",
    "    \n",
    "    # Data leakage check\n",
    "    print(f\"\\nDATA LEAKAGE PREVENTION:\")\n",
    "    print(f\"  ✓ STL parameters computed from training data only\")\n",
    "    print(f\"  ✓ Anomaly thresholds computed from training residuals only\")\n",
    "    print(f\"  ✓ ACF features computed from training data only\")\n",
    "    print(f\"  ✓ Test features derived using training parameters\")\n",
    "    \n",
    "    # Show sample of new features\n",
    "    new_feature_cols = [col for col in train_fe.columns if col not in train_df.columns]\n",
    "    if new_feature_cols:\n",
    "        print(f\"\\nSample new features:\")\n",
    "        for i, col in enumerate(new_feature_cols[:10]):\n",
    "            print(f\"  {i+1}. {col}\")\n",
    "        if len(new_feature_cols) > 10:\n",
    "            print(f\"  ... and {len(new_feature_cols) - 10} more\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    train_missing = train_fe.isnull().sum().sum()\n",
    "    test_missing = test_fe.isnull().sum().sum()\n",
    "    print(f\"\\nMissing values:\")\n",
    "    print(f\"  Training: {train_missing}\")\n",
    "    print(f\"  Test: {test_missing}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return train_fe, test_fe, {\n",
    "        'stl_params': stl_params,\n",
    "        'anomaly_params': anomaly_params, \n",
    "        'acf_params': acf_params\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAFE FEATURE ENGINEERING PIPELINE\n",
      "============================================================\n",
      "Training data: 1967-02-01 00:00:00 to 2019-12-01 00:00:00\n",
      "Test data: 2020-01-01 00:00:00 to 2025-05-01 00:00:00\n",
      "\n",
      "Processing 12 financial indicators...\n",
      "Anomaly detection for 9 columns...\n",
      "ACF features for 12 columns...\n",
      "\n",
      "1. STL Decomposition (Training Data Only)...\n",
      "   Processing 1_year_rate...\n",
      "   Processing 3_months_rate...\n",
      "   Processing 6_months_rate...\n",
      "   Processing CPI...\n",
      "   Processing INDPRO...\n",
      "   Processing 10_year_rate...\n",
      "   Processing share_price...\n",
      "   Processing unemployment_rate...\n",
      "   Processing PPI...\n",
      "   Processing OECD_CLI_index...\n",
      "   Processing CSI_index...\n",
      "   Processing gdp_per_capita...\n",
      "\n",
      "   Applying STL features to test data...\n",
      "\n",
      "2. Anomaly Detection (Training Data Only)...\n",
      "   Processing anomalies for INDPRO...\n",
      "   INDPRO: 9 anomalies in training, 35 in test\n",
      "   Processing anomalies for CPI...\n",
      "   CPI: 9 anomalies in training, 60 in test\n",
      "   Processing anomalies for unemployment_rate...\n",
      "   unemployment_rate: 12 anomalies in training, 33 in test\n",
      "   Processing anomalies for PPI...\n",
      "   PPI: 12 anomalies in training, 55 in test\n",
      "   Processing anomalies for share_price...\n",
      "   share_price: 17 anomalies in training, 61 in test\n",
      "   Processing anomalies for 1_year_rate...\n",
      "   1_year_rate: 14 anomalies in training, 57 in test\n",
      "   Processing anomalies for 3_months_rate...\n",
      "   3_months_rate: 15 anomalies in training, 57 in test\n",
      "   Processing anomalies for 6_months_rate...\n",
      "   6_months_rate: 14 anomalies in training, 57 in test\n",
      "   Processing anomalies for 10_year_rate...\n",
      "   10_year_rate: 11 anomalies in training, 39 in test\n",
      "\n",
      "3. ACF Features (Training Data Only)...\n",
      "   Computing ACF features for 1_year_rate...\n",
      "   Computing ACF features for 3_months_rate...\n",
      "   Computing ACF features for 6_months_rate...\n",
      "   Computing ACF features for CPI...\n",
      "   Computing ACF features for INDPRO...\n",
      "   Computing ACF features for 10_year_rate...\n",
      "   Computing ACF features for share_price...\n",
      "   Computing ACF features for unemployment_rate...\n",
      "   Computing ACF features for PPI...\n",
      "   Computing ACF features for OECD_CLI_index...\n",
      "   Computing ACF features for CSI_index...\n",
      "   Computing ACF features for gdp_per_capita...\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "============================================================\n",
      "Training data: 20 → 149 columns (+129)\n",
      "Test data: 20 → 149 columns (+129)\n",
      "\n",
      "Feature breakdown:\n",
      "  STL features: 48\n",
      "  Anomaly features: 9\n",
      "  ACF features: 84\n",
      "\n",
      "DATA LEAKAGE PREVENTION:\n",
      "  ✓ STL parameters computed from training data only\n",
      "  ✓ Anomaly thresholds computed from training residuals only\n",
      "  ✓ ACF features computed from training data only\n",
      "  ✓ Test features derived using training parameters\n",
      "\n",
      "Sample new features:\n",
      "  1. 1_year_rate_trend\n",
      "  2. 1_year_rate_seasonal\n",
      "  3. 1_year_rate_residual\n",
      "  4. 3_months_rate_trend\n",
      "  5. 3_months_rate_seasonal\n",
      "  6. 3_months_rate_residual\n",
      "  7. 6_months_rate_trend\n",
      "  8. 6_months_rate_seasonal\n",
      "  9. 6_months_rate_residual\n",
      "  10. CPI_trend\n",
      "  ... and 119 more\n",
      "\n",
      "Missing values:\n",
      "  Training: 87\n",
      "  Test: 11\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Apply safe feature engineering\n",
    "train_engineered, test_engineered, params = safe_feature_engineering_pipeline(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def add_fourier_features(train_df, test_df, date_col='date'):\n",
    "    \"\"\"\n",
    "    Add Fourier series features for seasonal patterns to both training and test data.\n",
    "    Prevents data leakage by using training data start date as reference for both datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_df : pd.DataFrame\n",
    "        Training dataset with date column\n",
    "    test_df : pd.DataFrame  \n",
    "        Test dataset with date column\n",
    "    date_col : str\n",
    "        Name of the date column (default: 'date')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (train_with_fourier, test_with_fourier, fourier_params)\n",
    "        - train_with_fourier: Training data with added Fourier features\n",
    "        - test_with_fourier: Test data with added Fourier features  \n",
    "        - fourier_params: Dictionary containing Fourier parameters for reference\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"FOURIER SERIES FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Make copies to avoid modifying originals\n",
    "    train_fourier = train_df.copy()\n",
    "    test_fourier = test_df.copy()\n",
    "    \n",
    "    # Ensure date columns are datetime\n",
    "    train_fourier[date_col] = pd.to_datetime(train_fourier[date_col])\n",
    "    test_fourier[date_col] = pd.to_datetime(test_fourier[date_col])\n",
    "    \n",
    "    # Get reference date from training data (CRITICAL for avoiding data leakage)\n",
    "    train_start_date = train_fourier[date_col].min()\n",
    "    \n",
    "    print(f\"Training period: {train_fourier[date_col].min()} to {train_fourier[date_col].max()}\")\n",
    "    print(f\"Test period: {test_fourier[date_col].min()} to {test_fourier[date_col].max()}\")\n",
    "    print(f\"Using training start date as reference: {train_start_date}\")\n",
    "    \n",
    "    # Initialize fourier parameters dictionary\n",
    "    fourier_params = {\n",
    "        'reference_date': train_start_date,\n",
    "        'features_added': []\n",
    "    }\n",
    "    \n",
    "    def add_fourier_terms(df, reference_date, K, seasonal_period, prefix):\n",
    "        \"\"\"\n",
    "        Add K pairs of Fourier terms (sin/cos) to dataframe\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            Dataframe to add features to\n",
    "        reference_date : pd.Timestamp\n",
    "            Reference date (t=0 point)\n",
    "        K : int\n",
    "            Number of Fourier pairs to generate\n",
    "        seasonal_period : float\n",
    "            Seasonal period in the same units as time calculation\n",
    "        prefix : str\n",
    "            Prefix for feature names\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate time index relative to reference date\n",
    "        # Convert to units of seasonal period\n",
    "        time_numeric = (df[date_col] - reference_date).dt.days / (365.25 / seasonal_period)\n",
    "        \n",
    "        features_added = []\n",
    "        \n",
    "        # Generate K pairs of Fourier terms\n",
    "        for k in range(1, K + 1):\n",
    "            # Frequency for this harmonic\n",
    "            freq = 2 * np.pi * k / seasonal_period\n",
    "            \n",
    "            # Feature names\n",
    "            sin_name = f'{prefix}_sin_{k}'\n",
    "            cos_name = f'{prefix}_cos_{k}'\n",
    "            \n",
    "            # Generate sin and cos features\n",
    "            df[sin_name] = np.sin(freq * time_numeric)\n",
    "            df[cos_name] = np.cos(freq * time_numeric)\n",
    "            \n",
    "            features_added.extend([sin_name, cos_name])\n",
    "        \n",
    "        return df, features_added\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. MONTHLY/ANNUAL SEASONALITY\n",
    "    # ==========================================\n",
    "    print(\"\\n1. Adding Monthly/Annual Fourier Features...\")\n",
    "    \n",
    "    K_monthly = 6  # 6 pairs = 12 features\n",
    "    seasonal_period_monthly = 12  # Monthly data, 12 months = 1 year\n",
    "    \n",
    "    train_fourier, monthly_features = add_fourier_terms(\n",
    "        train_fourier, train_start_date, K_monthly, seasonal_period_monthly, 'fourier_monthly'\n",
    "    )\n",
    "    \n",
    "    test_fourier, _ = add_fourier_terms(\n",
    "        test_fourier, train_start_date, K_monthly, seasonal_period_monthly, 'fourier_monthly'\n",
    "    )\n",
    "    \n",
    "    fourier_params['monthly'] = {\n",
    "        'K': K_monthly,\n",
    "        'seasonal_period': seasonal_period_monthly,\n",
    "        'features': monthly_features\n",
    "    }\n",
    "    fourier_params['features_added'].extend(monthly_features)\n",
    "    \n",
    "    print(f\"   Added {K_monthly} pairs ({len(monthly_features)} features) for monthly seasonality\")\n",
    "    print(f\"   Features: {monthly_features[:4]}...\" if len(monthly_features) > 4 else f\"   Features: {monthly_features}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. QUARTERLY SEASONALITY\n",
    "    # ==========================================\n",
    "    print(\"\\n2. Adding Quarterly Fourier Features...\")\n",
    "    \n",
    "    K_quarterly = 2  # 2 pairs = 4 features\n",
    "    seasonal_period_quarterly = 4  # 4 quarters per year\n",
    "    \n",
    "    train_fourier, quarterly_features = add_fourier_terms(\n",
    "        train_fourier, train_start_date, K_quarterly, seasonal_period_quarterly, 'fourier_quarterly'\n",
    "    )\n",
    "    \n",
    "    test_fourier, _ = add_fourier_terms(\n",
    "        test_fourier, train_start_date, K_quarterly, seasonal_period_quarterly, 'fourier_quarterly'\n",
    "    )\n",
    "    \n",
    "    fourier_params['quarterly'] = {\n",
    "        'K': K_quarterly,\n",
    "        'seasonal_period': seasonal_period_quarterly,\n",
    "        'features': quarterly_features\n",
    "    }\n",
    "    fourier_params['features_added'].extend(quarterly_features)\n",
    "    \n",
    "    print(f\"   Added {K_quarterly} pairs ({len(quarterly_features)} features) for quarterly seasonality\")\n",
    "    print(f\"   Features: {quarterly_features}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 3. BUSINESS CYCLE PATTERNS\n",
    "    # ==========================================\n",
    "    print(\"\\n3. Adding Business Cycle Fourier Features...\")\n",
    "    \n",
    "    K_business = 3  # 3 pairs = 6 features\n",
    "    seasonal_period_business = 48  # 4-year business cycle (48 months)\n",
    "    \n",
    "    train_fourier, business_features = add_fourier_terms(\n",
    "        train_fourier, train_start_date, K_business, seasonal_period_business, 'fourier_business'\n",
    "    )\n",
    "    \n",
    "    test_fourier, _ = add_fourier_terms(\n",
    "        test_fourier, train_start_date, K_business, seasonal_period_business, 'fourier_business'\n",
    "    )\n",
    "    \n",
    "    fourier_params['business_cycle'] = {\n",
    "        'K': K_business,\n",
    "        'seasonal_period': seasonal_period_business,\n",
    "        'features': business_features\n",
    "    }\n",
    "    fourier_params['features_added'].extend(business_features)\n",
    "    \n",
    "    print(f\"   Added {K_business} pairs ({len(business_features)} features) for business cycle patterns\")\n",
    "    print(f\"   Features: {business_features}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 4. LONG-TERM ECONOMIC CYCLES (OPTIONAL)\n",
    "    # ==========================================\n",
    "    print(\"\\n4. Adding Long-term Economic Cycle Features...\")\n",
    "    \n",
    "    K_longterm = 2  # 2 pairs = 4 features\n",
    "    seasonal_period_longterm = 120  # 10-year cycles (120 months)\n",
    "    \n",
    "    train_fourier, longterm_features = add_fourier_terms(\n",
    "        train_fourier, train_start_date, K_longterm, seasonal_period_longterm, 'fourier_longterm'\n",
    "    )\n",
    "    \n",
    "    test_fourier, _ = add_fourier_terms(\n",
    "        test_fourier, train_start_date, K_longterm, seasonal_period_longterm, 'fourier_longterm'\n",
    "    )\n",
    "    \n",
    "    fourier_params['longterm_cycle'] = {\n",
    "        'K': K_longterm,\n",
    "        'seasonal_period': seasonal_period_longterm,\n",
    "        'features': longterm_features\n",
    "    }\n",
    "    fourier_params['features_added'].extend(longterm_features)\n",
    "    \n",
    "    print(f\"   Added {K_longterm} pairs ({len(longterm_features)} features) for long-term cycles\")\n",
    "    print(f\"   Features: {longterm_features}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 5. SUMMARY AND VALIDATION\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FOURIER FEATURES SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    original_train_cols = len(train_df.columns)\n",
    "    original_test_cols = len(test_df.columns)\n",
    "    new_train_cols = len(train_fourier.columns)\n",
    "    new_test_cols = len(test_fourier.columns)\n",
    "    \n",
    "    total_fourier_features = len(fourier_params['features_added'])\n",
    "    \n",
    "    print(f\"Training data: {original_train_cols} → {new_train_cols} columns (+{new_train_cols - original_train_cols})\")\n",
    "    print(f\"Test data: {original_test_cols} → {new_test_cols} columns (+{new_test_cols - original_test_cols})\")\n",
    "    print(f\"Total Fourier features added: {total_fourier_features}\")\n",
    "    \n",
    "    print(f\"\\nFeature breakdown:\")\n",
    "    print(f\"  Monthly/Annual: {K_monthly} pairs ({K_monthly * 2} features)\")\n",
    "    print(f\"  Quarterly: {K_quarterly} pairs ({K_quarterly * 2} features)\")\n",
    "    print(f\"  Business Cycle: {K_business} pairs ({K_business * 2} features)\")\n",
    "    print(f\"  Long-term Cycle: {K_longterm} pairs ({K_longterm * 2} features)\")\n",
    "    \n",
    "    print(f\"\\nMathematical basis:\")\n",
    "    print(f\"  Monthly: sin/cos(2πkt/12) for k=1,2,...,{K_monthly}\")\n",
    "    print(f\"  Quarterly: sin/cos(2πkt/4) for k=1,2,...,{K_quarterly}\")\n",
    "    print(f\"  Business: sin/cos(2πkt/48) for k=1,2,...,{K_business}\")\n",
    "    print(f\"  Long-term: sin/cos(2πkt/120) for k=1,2,...,{K_longterm}\")\n",
    "    \n",
    "    print(f\"\\nDATA LEAKAGE PREVENTION:\")\n",
    "    print(f\"  ✓ All features use training start date as reference (t=0)\")\n",
    "    print(f\"  ✓ Fourier terms are deterministic functions of time only\")\n",
    "    print(f\"  ✓ No test data values used in feature computation\")\n",
    "    print(f\"  ✓ Test features are continuous extensions of training features\")\n",
    "    \n",
    "    # Sample feature values for verification\n",
    "    print(f\"\\nSample feature verification:\")\n",
    "    sample_train_features = train_fourier[fourier_params['features_added'][:4]].iloc[0].round(4)\n",
    "    sample_test_features = test_fourier[fourier_params['features_added'][:4]].iloc[0].round(4)\n",
    "    \n",
    "    print(f\"  First training sample: {dict(sample_train_features)}\")\n",
    "    print(f\"  First test sample: {dict(sample_test_features)}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    train_missing = train_fourier.isnull().sum().sum()\n",
    "    test_missing = test_fourier.isnull().sum().sum()\n",
    "    print(f\"\\nMissing values after Fourier features:\")\n",
    "    print(f\"  Training: {train_missing}\")\n",
    "    print(f\"  Test: {test_missing}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"FOURIER FEATURE ENGINEERING COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return train_fourier, test_fourier, fourier_params\n",
    "\n",
    "# Example usage and testing function\n",
    "def test_fourier_features():\n",
    "    \"\"\"Test the Fourier features function with sample data\"\"\"\n",
    "    \n",
    "    print(\"Testing Fourier features function...\")\n",
    "    \n",
    "    # Create sample data\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Training data (2 years)\n",
    "    train_dates = pd.date_range('2020-01-01', '2021-12-31', freq='M')\n",
    "    train_data = {\n",
    "        'date': train_dates,\n",
    "        'recession_probability': np.random.random(len(train_dates)),\n",
    "        'CPI': 100 + np.random.randn(len(train_dates)) * 2\n",
    "    }\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    \n",
    "    # Test data (6 months)\n",
    "    test_dates = pd.date_range('2022-01-01', '2022-06-30', freq='M')\n",
    "    test_data = {\n",
    "        'date': test_dates,\n",
    "        'recession_probability': np.random.random(len(test_dates)),\n",
    "        'CPI': 100 + np.random.randn(len(test_dates)) * 2\n",
    "    }\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    \n",
    "    # Apply Fourier features\n",
    "    train_with_fourier, test_with_fourier, params = add_fourier_features(train_engineered, test_engineered)\n",
    "    \n",
    "    print(f\"\\nTest completed successfully!\")\n",
    "    print(f\"Original columns: {list(train_df.columns)}\")\n",
    "    print(f\"New columns sample: {list(train_with_fourier.columns)[-8:]}\")\n",
    "    \n",
    "    return train_with_fourier, test_with_fourier, params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Fourier features function...\n",
      "============================================================\n",
      "FOURIER SERIES FEATURE ENGINEERING\n",
      "============================================================\n",
      "Training period: 1967-02-01 00:00:00 to 2019-12-01 00:00:00\n",
      "Test period: 2020-01-01 00:00:00 to 2025-05-01 00:00:00\n",
      "Using training start date as reference: 1967-02-01 00:00:00\n",
      "\n",
      "1. Adding Monthly/Annual Fourier Features...\n",
      "   Added 6 pairs (12 features) for monthly seasonality\n",
      "   Features: ['fourier_monthly_sin_1', 'fourier_monthly_cos_1', 'fourier_monthly_sin_2', 'fourier_monthly_cos_2']...\n",
      "\n",
      "2. Adding Quarterly Fourier Features...\n",
      "   Added 2 pairs (4 features) for quarterly seasonality\n",
      "   Features: ['fourier_quarterly_sin_1', 'fourier_quarterly_cos_1', 'fourier_quarterly_sin_2', 'fourier_quarterly_cos_2']\n",
      "\n",
      "3. Adding Business Cycle Fourier Features...\n",
      "   Added 3 pairs (6 features) for business cycle patterns\n",
      "   Features: ['fourier_business_sin_1', 'fourier_business_cos_1', 'fourier_business_sin_2', 'fourier_business_cos_2', 'fourier_business_sin_3', 'fourier_business_cos_3']\n",
      "\n",
      "4. Adding Long-term Economic Cycle Features...\n",
      "   Added 2 pairs (4 features) for long-term cycles\n",
      "   Features: ['fourier_longterm_sin_1', 'fourier_longterm_cos_1', 'fourier_longterm_sin_2', 'fourier_longterm_cos_2']\n",
      "\n",
      "============================================================\n",
      "FOURIER FEATURES SUMMARY\n",
      "============================================================\n",
      "Training data: 149 → 175 columns (+26)\n",
      "Test data: 149 → 175 columns (+26)\n",
      "Total Fourier features added: 26\n",
      "\n",
      "Feature breakdown:\n",
      "  Monthly/Annual: 6 pairs (12 features)\n",
      "  Quarterly: 2 pairs (4 features)\n",
      "  Business Cycle: 3 pairs (6 features)\n",
      "  Long-term Cycle: 2 pairs (4 features)\n",
      "\n",
      "Mathematical basis:\n",
      "  Monthly: sin/cos(2πkt/12) for k=1,2,...,6\n",
      "  Quarterly: sin/cos(2πkt/4) for k=1,2,...,2\n",
      "  Business: sin/cos(2πkt/48) for k=1,2,...,3\n",
      "  Long-term: sin/cos(2πkt/120) for k=1,2,...,2\n",
      "\n",
      "DATA LEAKAGE PREVENTION:\n",
      "  ✓ All features use training start date as reference (t=0)\n",
      "  ✓ Fourier terms are deterministic functions of time only\n",
      "  ✓ No test data values used in feature computation\n",
      "  ✓ Test features are continuous extensions of training features\n",
      "\n",
      "Sample feature verification:\n",
      "  First training sample: {'fourier_monthly_sin_1': 0.0, 'fourier_monthly_cos_1': 1.0, 'fourier_monthly_sin_2': 0.0, 'fourier_monthly_cos_2': 1.0}\n",
      "  First test sample: {'fourier_monthly_sin_1': -0.5121, 'fourier_monthly_cos_1': 0.859, 'fourier_monthly_sin_2': -0.8797, 'fourier_monthly_cos_2': 0.4756}\n",
      "\n",
      "Missing values after Fourier features:\n",
      "  Training: 87\n",
      "  Test: 11\n",
      "\n",
      "============================================================\n",
      "FOURIER FEATURE ENGINEERING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Test completed successfully!\n",
      "Original columns: ['date', 'recession_probability', 'CPI']\n",
      "New columns sample: ['fourier_business_sin_2', 'fourier_business_cos_2', 'fourier_business_sin_3', 'fourier_business_cos_3', 'fourier_longterm_sin_1', 'fourier_longterm_cos_1', 'fourier_longterm_sin_2', 'fourier_longterm_cos_2']\n"
     ]
    }
   ],
   "source": [
    "train_final, test_final, fourier_params = test_fourier_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "recession_probability\n",
      "1_month_recession_probability\n",
      "3_month_recession_probability\n",
      "6_month_recession_probability\n",
      "1_year_rate\n",
      "3_months_rate\n",
      "6_months_rate\n",
      "CPI\n",
      "INDPRO\n",
      "10_year_rate\n",
      "share_price\n",
      "unemployment_rate\n",
      "PPI\n",
      "OECD_CLI_index\n",
      "CSI_index\n",
      "Business_Quarter\n",
      "Month\n",
      "Country\n",
      "gdp_per_capita\n",
      "1_year_rate_trend\n",
      "1_year_rate_seasonal\n",
      "1_year_rate_residual\n",
      "3_months_rate_trend\n",
      "3_months_rate_seasonal\n",
      "3_months_rate_residual\n",
      "6_months_rate_trend\n",
      "6_months_rate_seasonal\n",
      "6_months_rate_residual\n",
      "CPI_trend\n",
      "CPI_seasonal\n",
      "CPI_residual\n",
      "INDPRO_trend\n",
      "INDPRO_seasonal\n",
      "INDPRO_residual\n",
      "10_year_rate_trend\n",
      "10_year_rate_seasonal\n",
      "10_year_rate_residual\n",
      "share_price_trend\n",
      "share_price_seasonal\n",
      "share_price_residual\n",
      "unemployment_rate_trend\n",
      "unemployment_rate_seasonal\n",
      "unemployment_rate_residual\n",
      "PPI_trend\n",
      "PPI_seasonal\n",
      "PPI_residual\n",
      "OECD_CLI_index_trend\n",
      "OECD_CLI_index_seasonal\n",
      "OECD_CLI_index_residual\n",
      "CSI_index_trend\n",
      "CSI_index_seasonal\n",
      "CSI_index_residual\n",
      "gdp_per_capita_trend\n",
      "gdp_per_capita_seasonal\n",
      "gdp_per_capita_residual\n",
      "INDPRO_anomaly\n",
      "CPI_anomaly\n",
      "unemployment_rate_anomaly\n",
      "PPI_anomaly\n",
      "share_price_anomaly\n",
      "1_year_rate_anomaly\n",
      "3_months_rate_anomaly\n",
      "6_months_rate_anomaly\n",
      "10_year_rate_anomaly\n",
      "1_year_rate_first_acf_original\n",
      "1_year_rate_sumsq_acf_original\n",
      "1_year_rate_first_acf_diff1\n",
      "1_year_rate_sumsq_acf_diff1\n",
      "1_year_rate_first_acf_diff2\n",
      "1_year_rate_sumsq_acf_diff2\n",
      "1_year_rate_seasonal_acf\n",
      "3_months_rate_first_acf_original\n",
      "3_months_rate_sumsq_acf_original\n",
      "3_months_rate_first_acf_diff1\n",
      "3_months_rate_sumsq_acf_diff1\n",
      "3_months_rate_first_acf_diff2\n",
      "3_months_rate_sumsq_acf_diff2\n",
      "3_months_rate_seasonal_acf\n",
      "6_months_rate_first_acf_original\n",
      "6_months_rate_sumsq_acf_original\n",
      "6_months_rate_first_acf_diff1\n",
      "6_months_rate_sumsq_acf_diff1\n",
      "6_months_rate_first_acf_diff2\n",
      "6_months_rate_sumsq_acf_diff2\n",
      "6_months_rate_seasonal_acf\n",
      "CPI_first_acf_original\n",
      "CPI_sumsq_acf_original\n",
      "CPI_first_acf_diff1\n",
      "CPI_sumsq_acf_diff1\n",
      "CPI_first_acf_diff2\n",
      "CPI_sumsq_acf_diff2\n",
      "CPI_seasonal_acf\n",
      "INDPRO_first_acf_original\n",
      "INDPRO_sumsq_acf_original\n",
      "INDPRO_first_acf_diff1\n",
      "INDPRO_sumsq_acf_diff1\n",
      "INDPRO_first_acf_diff2\n",
      "INDPRO_sumsq_acf_diff2\n",
      "INDPRO_seasonal_acf\n",
      "10_year_rate_first_acf_original\n",
      "10_year_rate_sumsq_acf_original\n",
      "10_year_rate_first_acf_diff1\n",
      "10_year_rate_sumsq_acf_diff1\n",
      "10_year_rate_first_acf_diff2\n",
      "10_year_rate_sumsq_acf_diff2\n",
      "10_year_rate_seasonal_acf\n",
      "share_price_first_acf_original\n",
      "share_price_sumsq_acf_original\n",
      "share_price_first_acf_diff1\n",
      "share_price_sumsq_acf_diff1\n",
      "share_price_first_acf_diff2\n",
      "share_price_sumsq_acf_diff2\n",
      "share_price_seasonal_acf\n",
      "unemployment_rate_first_acf_original\n",
      "unemployment_rate_sumsq_acf_original\n",
      "unemployment_rate_first_acf_diff1\n",
      "unemployment_rate_sumsq_acf_diff1\n",
      "unemployment_rate_first_acf_diff2\n",
      "unemployment_rate_sumsq_acf_diff2\n",
      "unemployment_rate_seasonal_acf\n",
      "PPI_first_acf_original\n",
      "PPI_sumsq_acf_original\n",
      "PPI_first_acf_diff1\n",
      "PPI_sumsq_acf_diff1\n",
      "PPI_first_acf_diff2\n",
      "PPI_sumsq_acf_diff2\n",
      "PPI_seasonal_acf\n",
      "OECD_CLI_index_first_acf_original\n",
      "OECD_CLI_index_sumsq_acf_original\n",
      "OECD_CLI_index_first_acf_diff1\n",
      "OECD_CLI_index_sumsq_acf_diff1\n",
      "OECD_CLI_index_first_acf_diff2\n",
      "OECD_CLI_index_sumsq_acf_diff2\n",
      "OECD_CLI_index_seasonal_acf\n",
      "CSI_index_first_acf_original\n",
      "CSI_index_sumsq_acf_original\n",
      "CSI_index_first_acf_diff1\n",
      "CSI_index_sumsq_acf_diff1\n",
      "CSI_index_first_acf_diff2\n",
      "CSI_index_sumsq_acf_diff2\n",
      "CSI_index_seasonal_acf\n",
      "gdp_per_capita_first_acf_original\n",
      "gdp_per_capita_sumsq_acf_original\n",
      "gdp_per_capita_first_acf_diff1\n",
      "gdp_per_capita_sumsq_acf_diff1\n",
      "gdp_per_capita_first_acf_diff2\n",
      "gdp_per_capita_sumsq_acf_diff2\n",
      "gdp_per_capita_seasonal_acf\n",
      "fourier_monthly_sin_1\n",
      "fourier_monthly_cos_1\n",
      "fourier_monthly_sin_2\n",
      "fourier_monthly_cos_2\n",
      "fourier_monthly_sin_3\n",
      "fourier_monthly_cos_3\n",
      "fourier_monthly_sin_4\n",
      "fourier_monthly_cos_4\n",
      "fourier_monthly_sin_5\n",
      "fourier_monthly_cos_5\n",
      "fourier_monthly_sin_6\n",
      "fourier_monthly_cos_6\n",
      "fourier_quarterly_sin_1\n",
      "fourier_quarterly_cos_1\n",
      "fourier_quarterly_sin_2\n",
      "fourier_quarterly_cos_2\n",
      "fourier_business_sin_1\n",
      "fourier_business_cos_1\n",
      "fourier_business_sin_2\n",
      "fourier_business_cos_2\n",
      "fourier_business_sin_3\n",
      "fourier_business_cos_3\n",
      "fourier_longterm_sin_1\n",
      "fourier_longterm_cos_1\n",
      "fourier_longterm_sin_2\n",
      "fourier_longterm_cos_2\n"
     ]
    }
   ],
   "source": [
    "for col in train_final.columns:\n",
    "    print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Columns only in train_final:\n",
      "None\n",
      "\n",
      "🔹 Columns only in test_final:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Columns that are in train_final but not in test_final\n",
    "train_only = set(train_final.columns) - set(test_final.columns)\n",
    "\n",
    "# Columns that are in test_final but not in train_final\n",
    "test_only = set(test_final.columns) - set(train_final.columns)\n",
    "\n",
    "print(\"🔹 Columns only in train_final:\")\n",
    "print(train_only if train_only else \"None\")\n",
    "\n",
    "print(\"\\n🔹 Columns only in test_final:\")\n",
    "print(test_only if test_only else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final.to_csv(\"../data/fix/feature_engineered_recession_test.csv\", index=False)\n",
    "train_final.to_csv(\"../data/fix/feature_engineered_recession_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 175)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 175)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
