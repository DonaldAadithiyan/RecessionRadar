{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from statsmodels.tsa.api import VAR\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reduced_df = pd.read_csv('../data/feature-engineered/df_2_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 52)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_reduced_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                                      object\n",
       "1_year_rate                              float64\n",
       "3_months_rate                            float64\n",
       "6_months_rate                            float64\n",
       "CPI                                      float64\n",
       "INDPRO                                   float64\n",
       "10_year_rate                             float64\n",
       "share_price                              float64\n",
       "unemployment_rate                        float64\n",
       "PPI                                      float64\n",
       "OECD_CLI_index                           float64\n",
       "CSI_index                                float64\n",
       "gdp_per_capita                           float64\n",
       "seasonally_adjusted_unemployment_rate    float64\n",
       "CPI_trend                                float64\n",
       "CPI_sumsq_acf_diff1                      float64\n",
       "seasonally_adjusted_CPI                  float64\n",
       "INDPRO_trend                             float64\n",
       "seasonally_adjusted_INDPRO               float64\n",
       "INDPRO_acf1_original                     float64\n",
       "CPI_sumsq_acf_original                   float64\n",
       "INDPRO_sumsq_acf_original                float64\n",
       "CPI_acf1_original                        float64\n",
       "recession_probability                    float64\n",
       "PPI_trend                                float64\n",
       "1_year_rate_acf1_diff1                   float64\n",
       "3_months_rate_acf1_original              float64\n",
       "gdp_per_capita_acf1_original             float64\n",
       "share_price_acf1_original                float64\n",
       "seasonally_adjusted_PPI                  float64\n",
       "6_months_rate_sumsq_acf_diff1            float64\n",
       "gdp_per_capita_sumsq_acf_original        float64\n",
       "Year                                       int64\n",
       "share_price_sumsq_acf_original           float64\n",
       "CPI_acf1_diff2                           float64\n",
       "3_months_rate_sumsq_acf_original         float64\n",
       "CPI_acf1_diff1                           float64\n",
       "6_months_rate_sumsq_acf_original         float64\n",
       "6_months_rate_acf1_original              float64\n",
       "10_year_rate_acf1_original               float64\n",
       "10_year_rate_sumsq_acf_original          float64\n",
       "3_months_rate_acf1_diff1                 float64\n",
       "1_year_rate_sumsq_acf_original           float64\n",
       "1_year_rate_acf1_original                float64\n",
       "unemployment_rate_trend                  float64\n",
       "INDPRO_acf1_diff2                        float64\n",
       "3_month_recession_probability            float64\n",
       "OECD_CLI_index_sumsq_acf_original        float64\n",
       "recession_probability.1                  float64\n",
       "1_month_recession_probability            float64\n",
       "3_month_recession_probability.1          float64\n",
       "6_month_recession_probability            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_reduced_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reduced_df = dim_reduced_df.drop(['3_month_recession_probability.1', 'recession_probability.1'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reduced_df.to_csv('../data/feature-engineered/df_2_reduced_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- CONFIG ----\n",
    "TARGET_COLS = [\n",
    "    'share_price', 'gdp_per_capita'\n",
    "]\n",
    "\n",
    "RECESSION_COLS = [\n",
    "    'recession_probability','1_month_recession_probability',\n",
    "    '3_month_recession_probability','6_month_recession_probability'\n",
    "]\n",
    "\n",
    "SPLIT_DATE = '2020-01-01'   # test set starts here\n",
    "\n",
    "# -------------------------\n",
    "# ---- Utility / Pipeline\n",
    "# -------------------------\n",
    "def make_splits(df, split_date=SPLIT_DATE):\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').set_index('date')\n",
    "\n",
    "    train = df.loc[df.index < split_date].copy()\n",
    "    test  = df.loc[df.index >= split_date].copy()\n",
    "\n",
    "    print(f\"Train: {train.index.min().date()} → {train.index.max().date()} ({len(train)})\")\n",
    "    print(f\"Test : {test.index.min().date()} → {test.index.max().date()} ({len(test)})\")\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def build_exog(df, target_cols=TARGET_COLS, recession_cols=RECESSION_COLS):\n",
    "    # Exogenous features = everything except targets + recession cols\n",
    "    drop_cols = set(target_cols) | set(recession_cols)\n",
    "    exog_cols = [c for c in df.columns if c not in drop_cols]\n",
    "    return df[exog_cols].copy(), exog_cols\n",
    "\n",
    "\n",
    "def clean_exog(train_exog, test_exog):\n",
    "    # Fill, replace inf, align columns in same order\n",
    "    def _clean(X):\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        X = X.ffill().bfill()\n",
    "        return X\n",
    "\n",
    "    train_exog = _clean(train_exog)\n",
    "    test_exog  = _clean(test_exog)\n",
    "\n",
    "    # Remove constant columns (no variance) using train only\n",
    "    keep = train_exog.nunique() > 1\n",
    "    train_exog = train_exog.loc[:, keep]\n",
    "    test_exog  = test_exog[train_exog.columns]  # same columns/order\n",
    "\n",
    "    # Final safety fill\n",
    "    test_exog  = test_exog.fillna(method='ffill').fillna(method='bfill')\n",
    "    train_exog = train_exog.fillna(method='ffill').fillna(method='bfill')\n",
    "    return train_exog, test_exog\n",
    "\n",
    "\n",
    "def difference_targets(train_y, test_y):\n",
    "    \"\"\" First-difference targets for stationarity and keep last train level to invert later. \"\"\"\n",
    "    dy_train = train_y.diff().dropna()\n",
    "    # For test, we will forecast differences; inversion uses last train level\n",
    "    last_train_level = train_y.iloc[-1]\n",
    "    return dy_train, last_train_level\n",
    "\n",
    "\n",
    "def invert_differences(diff_forecast_df, last_train_level):\n",
    "    \"\"\" Convert differenced forecasts back to levels via cumulative sum. \"\"\"\n",
    "    # cumulative sum across time, add last_train_level (broadcast on columns)\n",
    "    levels = diff_forecast_df.cumsum()\n",
    "    for col in levels.columns:\n",
    "        levels[col] = last_train_level[col] + levels[col]\n",
    "    return levels\n",
    "\n",
    "\n",
    "def fit_varmax_and_forecast(train_y, test_h, train_exog=None, test_exog=None, order_tuple=(1,0)):\n",
    "    \"\"\"\n",
    "    Fit VARMAX on (train_y) with optional exog, forecast 'test_h' steps ahead,\n",
    "    return forecasts (DataFrame) and the fitted results object.\n",
    "\n",
    "    order_tuple is (p, q)\n",
    "    \"\"\"\n",
    "    p, q = order_tuple\n",
    "    if train_exog is not None and test_exog is not None:\n",
    "        model = VARMAX(endog=train_y, exog=train_exog, order=(p, q), trend='n')\n",
    "        res = model.fit(maxiter=500, disp=False)\n",
    "        fcast = res.get_forecast(steps=test_h, exog=test_exog).predicted_mean\n",
    "    else:\n",
    "        model = VARMAX(endog=train_y, order=(p, q), trend='n')\n",
    "        res = model.fit(maxiter=500, disp=False)\n",
    "        fcast = res.get_forecast(steps=test_h).predicted_mean\n",
    "\n",
    "    # Build a sensible index for the forecast: start at next period after train_y\n",
    "    freq = pd.infer_freq(train_y.index)\n",
    "    if freq is None:\n",
    "        # fallback: use train frequency as difference\n",
    "        diff = train_y.index[1] - train_y.index[0]\n",
    "        start = train_y.index[-1] + diff\n",
    "        fcast.index = pd.date_range(start=start, periods=test_h, freq=diff)\n",
    "    else:\n",
    "        start = train_y.index[-1] + pd.tseries.frequencies.to_offset(freq)\n",
    "        fcast.index = pd.date_range(start=start, periods=test_h, freq=freq)\n",
    "\n",
    "    fcast.columns = train_y.columns\n",
    "    return fcast, res\n",
    "\n",
    "\n",
    "def fit_var_fallback(train_y, test_h, order=2):\n",
    "    \"\"\"\n",
    "    Fallback: plain VAR on (train_y) (no exog).\n",
    "    \"\"\"\n",
    "    var_model = VAR(train_y)\n",
    "    res = var_model.fit(order)\n",
    "    fcast_vals = res.forecast(train_y.values[-order:], steps=test_h)\n",
    "    freq = pd.infer_freq(train_y.index)\n",
    "    if freq is None:\n",
    "        diff = train_y.index[1] - train_y.index[0]\n",
    "        start = train_y.index[-1] + diff\n",
    "        index = pd.date_range(start=start, periods=test_h, freq=diff)\n",
    "    else:\n",
    "        start = train_y.index[-1] + pd.tseries.frequencies.to_offset(freq)\n",
    "        index = pd.date_range(start=start, periods=test_h, freq=freq)\n",
    "\n",
    "    fcast = pd.DataFrame(fcast_vals, index=index, columns=train_y.columns)\n",
    "    return fcast, res\n",
    "\n",
    "\n",
    "def evaluate_forecasts(actual_df, pred_df):\n",
    "    \"\"\" Return MAE/RMSE/MAPE per series. \"\"\"\n",
    "    common = actual_df.index.intersection(pred_df.index)\n",
    "    A = actual_df.loc[common]\n",
    "    P = pred_df.loc[common]\n",
    "\n",
    "    metrics = []\n",
    "    for col in A.columns:\n",
    "        a = A[col].astype(float)\n",
    "        p = P[col].astype(float)\n",
    "        mae  = np.mean(np.abs(p - a))\n",
    "        rmse = np.sqrt(np.mean((p - a)**2))\n",
    "        # avoid division by zero in MAPE\n",
    "        denom = np.where(a == 0, np.nan, np.abs(a))\n",
    "        mape = np.nanmean(np.abs((a - p) / denom)) * 100\n",
    "        metrics.append({'series': col, 'MAE': mae, 'RMSE': rmse, 'MAPE_%': mape})\n",
    "    return pd.DataFrame(metrics).set_index('series')\n",
    "\n",
    "# -------------------------\n",
    "# ---- Metaheuristic helpers (lightweight implementations)\n",
    "# -------------------------\n",
    "def fitness_VARMAX(order_tuple, train_y, val_y, train_exog=None, val_exog=None):\n",
    "    \"\"\"\n",
    "    Fit VARMAX(order_tuple) on train_y (and train_exog) and compute RMSE on val_y.\n",
    "    Return RMSE (lower is better). If fit fails, return large penalty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        forecast, _ = fit_varmax_and_forecast(\n",
    "            train_y=train_y,\n",
    "            test_h=len(val_y),\n",
    "            train_exog=train_exog,\n",
    "            test_exog=val_exog,\n",
    "            order_tuple=order_tuple\n",
    "        )\n",
    "        # Align indices (forecast index may differ but shapes match)\n",
    "        # compute RMSE across all series averaged\n",
    "        # if shape mismatch, treat as failure\n",
    "        if forecast.shape != val_y.shape:\n",
    "            return np.inf\n",
    "        rmse = np.sqrt(((val_y.values - forecast.values) ** 2).mean())\n",
    "        return float(rmse)\n",
    "    except Exception:\n",
    "        return np.inf\n",
    "\n",
    "\n",
    "def grey_wolf_optimize_p(train_y, val_y, train_exog=None, val_exog=None,\n",
    "                         p_min=1, p_max=5, pop_size=6, max_iters=10):\n",
    "    \"\"\"\n",
    "    Simple integer Grey Wolf Optimizer to search p in [p_min, p_max].\n",
    "    Returns best_p (int).\n",
    "    \"\"\"\n",
    "    # Initialize wolves randomly (integers)\n",
    "    wolves = np.random.randint(p_min, p_max+1, size=pop_size)\n",
    "    fitness = np.array([fitness_VARMAX((int(w), 0), train_y, val_y, train_exog, val_exog) for w in wolves])\n",
    "\n",
    "    # ensure we have finite values\n",
    "    for it in range(max_iters):\n",
    "        print(f\"[GWO] Iteration {it+1}/{max_iters}...\")\n",
    "        # sort wolves by fitness (lower better)\n",
    "        idx = np.argsort(fitness)\n",
    "        wolves = wolves[idx]\n",
    "        fitness = fitness[idx]\n",
    "\n",
    "        alpha, beta, delta = wolves[0], wolves[1], wolves[2]\n",
    "        print(f\"[GWO] Alpha={alpha}, Beta={beta}, Delta={delta}, Best fitness={fitness[0]:.4f}\")\n",
    "\n",
    "        # update positions (simple heuristic in integer space)\n",
    "        new_wolves = []\n",
    "        for w in wolves:\n",
    "            # create candidate by combining alpha/beta/delta with small random step\n",
    "            r1, r2 = np.random.rand(), np.random.rand()\n",
    "            A1 = 2 * r1 - 1\n",
    "            C1 = 2 * r2\n",
    "            candidate = int(np.round(alpha - A1 * abs(C1 * alpha - w)))\n",
    "\n",
    "            # move towards beta, delta occasionally\n",
    "            if np.random.rand() < 0.3:\n",
    "                candidate = int(np.round((candidate + beta)/2))\n",
    "            if np.random.rand() < 0.2:\n",
    "                candidate = int(np.round((candidate + delta)/2))\n",
    "\n",
    "            # clamp to bounds\n",
    "            candidate = max(p_min, min(p_max, candidate))\n",
    "            new_wolves.append(candidate)\n",
    "\n",
    "        wolves = np.array(new_wolves)\n",
    "        fitness = np.array([fitness_VARMAX((int(w), 0), train_y, val_y, train_exog, val_exog) for w in wolves])\n",
    "\n",
    "    # final pick best\n",
    "    best_idx = int(np.argmin(fitness))\n",
    "    print(f\"[GWO] Optimization finished. Best p={wolves[best_idx]} with fitness={fitness[best_idx]:.4f}\")\n",
    "    return int(wolves[best_idx])\n",
    "\n",
    "\n",
    "def coot_optimize_q(fixed_p, train_y, val_y, train_exog=None, val_exog=None,\n",
    "                    q_min=0, q_max=5, pop_size=6, max_iters=10):\n",
    "    \"\"\"\n",
    "    Simple Coot-like optimizer to search q in [q_min, q_max] given fixed p.\n",
    "    Returns best_q (int).\n",
    "    \"\"\"\n",
    "    # init population\n",
    "    coots = np.random.randint(q_min, q_max+1, size=pop_size)\n",
    "    fitness = np.array([fitness_VARMAX((fixed_p, int(c)), train_y, val_y, train_exog, val_exog) for c in coots])\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        print(f\"[COA] Iteration {it+1}/{max_iters}...\")\n",
    "        idx = np.argsort(fitness)\n",
    "        coots = coots[idx]\n",
    "        fitness = fitness[idx]\n",
    "        best = coots[0]\n",
    "        print(f\"[COA] Best q={best}, fitness={fitness[0]:.4f}\")\n",
    "\n",
    "\n",
    "        # Coot behaviors: random walk, group follow, leader follow, dive\n",
    "        new_coots = []\n",
    "        for c in coots:\n",
    "            if np.random.rand() < 0.4:\n",
    "                # small random step around best\n",
    "                candidate = best + np.random.randint(-1, 2)\n",
    "            elif np.random.rand() < 0.2:\n",
    "                # move towards group mean\n",
    "                candidate = int(round(np.mean(coots)))\n",
    "            else:\n",
    "                # exploratory jump\n",
    "                candidate = c + np.random.randint(-2, 3)\n",
    "\n",
    "            candidate = max(q_min, min(q_max, candidate))\n",
    "            new_coots.append(candidate)\n",
    "\n",
    "        coots = np.array(new_coots)\n",
    "        fitness = np.array([fitness_VARMAX((fixed_p, int(c)), train_y, val_y, train_exog, val_exog) for c in coots])\n",
    "\n",
    "    best_idx = int(np.argmin(fitness))\n",
    "    print(f\"[COA] Optimization finished. Best q={coots[best_idx]} with fitness={fitness[best_idx]:.4f}\")\n",
    "    return int(coots[best_idx])\n",
    "\n",
    "\n",
    "def optimize_orders_dual_metaheuristic(train_y_opt, val_y, train_exog_opt=None, val_exog=None,\n",
    "                                       p_range=(1,5), q_range=(0,5)):\n",
    "    \"\"\"\n",
    "    Use GWO to find p, then COA to find q (given p).\n",
    "    p_range and q_range are tuples (min, max).\n",
    "    \"\"\"\n",
    "    p_min, p_max = p_range\n",
    "    q_min, q_max = q_range\n",
    "\n",
    "    print(\"Running Grey Wolf Optimizer (GWO) to search AR order p ...\")\n",
    "    best_p = grey_wolf_optimize_p(train_y_opt, val_y, train_exog_opt, val_exog,\n",
    "                                  p_min=p_min, p_max=p_max, pop_size=8, max_iters=12)\n",
    "    print(f\"GWO found p = {best_p}\")\n",
    "\n",
    "    print(\"Running Coot Optimization (COA) to search MA order q (conditional on best p) ...\")\n",
    "    best_q = coot_optimize_q(best_p, train_y_opt, val_y, train_exog_opt, val_exog,\n",
    "                             q_min=q_min, q_max=q_max, pop_size=8, max_iters=12)\n",
    "    print(f\"COA found q = {best_q}\")\n",
    "\n",
    "    return best_p, best_q\n",
    "\n",
    "# -------------------------\n",
    "# ---- Main pipeline (modified)\n",
    "# -------------------------\n",
    "def run_multivariate_pipeline_meta(df, plot_all=False,\n",
    "                                   val_fraction=0.20,\n",
    "                                   p_search_range=(1,5),\n",
    "                                   q_search_range=(0,5)):\n",
    "    # 1) Split\n",
    "    train, test = make_splits(df)\n",
    "\n",
    "    # 2) Build Y (targets) and exog (features)\n",
    "    train_y = train[TARGET_COLS].copy()\n",
    "    test_y  = test[TARGET_COLS].copy()\n",
    "\n",
    "    train_exog_raw, exog_cols = build_exog(train)\n",
    "    test_exog_raw  = test[exog_cols].copy()\n",
    "\n",
    "    # 3) Clean exog and align\n",
    "    train_exog, test_exog = clean_exog(train_exog_raw, test_exog_raw)\n",
    "\n",
    "    # 4) Optionally difference targets for stationarity (here we skip differencing to keep original scale)\n",
    "    dy_train = train_y\n",
    "    last_train_level = None\n",
    "\n",
    "    # 5) Align exog rows with train_y index\n",
    "    train_exog_aligned = train_exog.loc[dy_train.index]\n",
    "\n",
    "    # ---- Split train into optimization-train and validation for hyperparameter search ----\n",
    "    val_size = max(1, int(len(dy_train) * val_fraction))\n",
    "    train_y_opt = dy_train.iloc[:-val_size]\n",
    "    val_y = dy_train.iloc[-val_size:]\n",
    "\n",
    "    train_exog_opt = train_exog_aligned.iloc[:-val_size] if train_exog_aligned is not None else None\n",
    "    val_exog = train_exog_aligned.iloc[-val_size:] if train_exog_aligned is not None else None\n",
    "\n",
    "    # 6) Optimize (p,q) using dual metaheuristic on the small validation split\n",
    "    try:\n",
    "        best_p, best_q = optimize_orders_dual_metaheuristic(\n",
    "            train_y_opt, val_y,\n",
    "            train_exog_opt, val_exog,\n",
    "            p_range=p_search_range, q_range=q_search_range\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Metaheuristic optimization failed: {e}. Falling back to defaults p=1,q=0\")\n",
    "        best_p, best_q = 1, 0\n",
    "\n",
    "    print(f\"Best VARMAX order found by metaheuristics: p={best_p}, q={best_q}\")\n",
    "\n",
    "    # 7) Fit final VARMAX with optimized orders on the entire training set and forecast test\n",
    "    steps = len(test)\n",
    "    try:\n",
    "        final_forecast, fitted_model = fit_varmax_and_forecast(\n",
    "            train_y=dy_train,\n",
    "            test_h=steps,\n",
    "            train_exog=train_exog_aligned,\n",
    "            test_exog=test_exog,\n",
    "            order_tuple=(best_p, best_q)\n",
    "        )\n",
    "        used_model = f\"Metaheuristic-optimized VARMAX (order=({best_p},{best_q}))\"\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Final VARMAX fit failed ({e}). Falling back to VAR (no exog).\")\n",
    "        final_forecast, fitted_model = fit_var_fallback(dy_train, steps)\n",
    "        used_model = \"VAR (no exogenous features)\"\n",
    "\n",
    "    # if we had differenced earlier we'd invert here; we skipped differencing so keep as-is\n",
    "    level_forecast = final_forecast\n",
    "\n",
    "    # Align forecast index with test index (force)\n",
    "    level_forecast.index = test_y.index\n",
    "\n",
    "    # 8) Evaluate against actual test levels\n",
    "    metrics = evaluate_forecasts(test_y, level_forecast)\n",
    "\n",
    "    # 9) Join predictions with actuals for convenience\n",
    "    out = test_y.join(level_forecast.add_suffix('_pred'), how='left')\n",
    "\n",
    "    print(f\"\\nModel used: {used_model}\")\n",
    "    print(\"\\nForecast metrics (test period):\")\n",
    "    print(metrics.round(4))\n",
    "\n",
    "    # plot residuals if possible\n",
    "    try:\n",
    "        residuals = fitted_model.resid\n",
    "        residuals.plot(subplots=True, figsize=(12,8))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 10) Optional plotting of series\n",
    "    if plot_all:\n",
    "        for series in TARGET_COLS:\n",
    "            plt.figure(figsize=(8,4))\n",
    "            plt.plot(train.index, train[series], label=f\"{series} (Train)\")\n",
    "            plt.plot(test.index, test[series], label=f\"{series} (Test)\", color=\"blue\")\n",
    "            plt.plot(out.index, out[f\"{series}_pred\"], label=f\"{series} (Forecast)\", color=\"orange\")\n",
    "            plt.title(f\"{series}: Actual vs Forecast\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Export fitted model as pickle\n",
    "    try:\n",
    "        with open(\"VARMAX_final_model.pkl\", \"wb\") as f:\n",
    "            pickle.dump(fitted_model, f)\n",
    "        print(\"✅ Model exported to VARMAX_final_model.pkl\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not export model: {e}\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        'predictions_vs_actuals': out,\n",
    "        'metrics': metrics,\n",
    "        'model_summary': str(fitted_model.summary()) if hasattr(fitted_model, 'summary') else None,\n",
    "        'best_order': (best_p, best_q)\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Example usage (uncomment and point to your df)\n",
    "# -------------------------\n",
    "# df = pd.read_csv(\"../data/feature-engineered/recession_probability.csv\")\n",
    "# result = run_multivariate_pipeline_meta(df, plot_all=True)\n",
    "# print(result['best_order'])\n",
    "# print(result['metrics'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1967-02-01 → 2019-12-01 (635)\n",
      "Test : 2020-01-01 → 2025-05-01 (65)\n",
      "Running Grey Wolf Optimizer (GWO) to search AR order p ...\n",
      "[GWO] Iteration 1/12...\n",
      "[GWO] Alpha=4, Beta=4, Delta=3, Best fitness=3064.0225\n",
      "[GWO] Iteration 2/12...\n",
      "[GWO] Alpha=5, Beta=5, Delta=4, Best fitness=3054.7552\n",
      "[GWO] Iteration 3/12...\n",
      "[GWO] Alpha=5, Beta=5, Delta=5, Best fitness=3054.7552\n"
     ]
    }
   ],
   "source": [
    "results = run_multivariate_pipeline_meta(dim_reduced_df)\n",
    "preds = results['predictions_vs_actuals']   # contains actuals + *_pred columns for all 12 series\n",
    "metrics = results['metrics']                # MAE/RMSE/MAPE per series\n",
    "print(results['model_summary'])             # Optional: model summary\n",
    "\n",
    "for col in preds.columns:\n",
    "    if not col.endswith(\"_pred\"):  # Only pick actual series\n",
    "        series = col\n",
    "        if f\"{series}_pred\" in preds.columns:  # Ensure prediction exists\n",
    "            preds[[series, f\"{series}_pred\"]].plot(\n",
    "                title=f\"{series}: Actual vs Forecast (Test)\",\n",
    "                figsize=(8, 5)\n",
    "            )\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(series)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
